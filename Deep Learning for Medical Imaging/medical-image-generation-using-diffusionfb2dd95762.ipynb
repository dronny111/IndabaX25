{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1LfsGqCPgqXTb47fHjD1krY8qSZTIqTsf","timestamp":1746033866095},{"file_id":"https://github.com/reshalfahsi/medical-image-generation/blob/master/Medical_Image_Generation_Using_Diffusion_Model.ipynb","timestamp":1745384577218}]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1099232,"sourceType":"datasetVersion","datasetId":614679}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":211.123357,"end_time":"2025-05-26T16:54:29.811007","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-05-26T16:50:58.687650","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0d2b2a977cbd4e718d34cea0c5b377ad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"3e3f936918e1400c9e8a5b93e1396d2c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"447e3cdd35b74ab3afc9bd0da2d3a5c7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7de24e9998af4f5bb515409ab0f33c9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e3f936918e1400c9e8a5b93e1396d2c","placeholder":"‚Äã","style":"IPY_MODEL_e802b91b6f5a4f04b284c592d82c50a0","value":"‚Äá0/2‚Äá[00:00&lt;?,‚Äá?it/s]"}},"9831bac7d8de463ca7bfeae8a1210b7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_447e3cdd35b74ab3afc9bd0da2d3a5c7","placeholder":"‚Äã","style":"IPY_MODEL_c056b2af805d4a279e1f597938230838","value":"Sanity‚ÄáChecking‚ÄáDataLoader‚Äá0:‚Äá‚Äá‚Äá0%"}},"c056b2af805d4a279e1f597938230838":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cce2910ed1c543be9b2a90c8b9483da4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9831bac7d8de463ca7bfeae8a1210b7e","IPY_MODEL_fde30a38b8624541aa9891eb89e1b731","IPY_MODEL_7de24e9998af4f5bb515409ab0f33c9b"],"layout":"IPY_MODEL_0d2b2a977cbd4e718d34cea0c5b377ad"}},"d3810aa0d19b4d12899e4b5ee2d8df6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ddfe9b8f3ea94ed9a46da0092cb6bdb5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e802b91b6f5a4f04b284c592d82c50a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fde30a38b8624541aa9891eb89e1b731":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddfe9b8f3ea94ed9a46da0092cb6bdb5","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d3810aa0d19b4d12899e4b5ee2d8df6c","value":0}}}}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"565d0e3e","cell_type":"markdown","source":"# **Medical Image Generation with Diffusion Models**","metadata":{"id":"nxLWNsrydEIG","papermill":{"duration":0.015066,"end_time":"2025-05-26T16:51:05.241584","exception":false,"start_time":"2025-05-26T16:51:05.226518","status":"completed"},"tags":[]}},{"id":"b1a38d15-9bfb-4886-a8e5-1766061b54f4","cell_type":"markdown","source":"## Table of Contents\n1. [Introduction to Diffusion Models](#introduction)\n2. [Prerequisites](#prerequisites)\n3. [Dataset Preparation](#dataset)\n4. [Coding the Diffusion Model](#coding-diffusion)\n5. [Training](#training)\n6. [Testing](#testing)\n7. [Inference](#inference)","metadata":{}},{"id":"56f116ff-9de9-42fe-9103-7339d2430c27","cell_type":"markdown","source":"## üß™ Introduction to Diffusion Models\n<a id=\"introduction\"></a>\n\nIn recent years, **diffusion models** have emerged as a powerful class of generative models, rivaling and even surpassing traditional methods like GANs and VAEs in image synthesis tasks. These models generate high-quality images by learning to gradually reverse a noise process‚Äîstarting from random noise and iteratively denoising it to recover realistic data samples.\n\nThis notebook will walk you through the **foundational components of a basic diffusion model**, starting from the **forward noising process**, moving to the **denoising network architecture**, and finally arriving at the **sampling process** used to generate new images. You will get hands-on experience coding the key parts of a DDPM (Denoising Diffusion Probabilistic Model), helping you build both intuition and implementation skills.\n\nWe'll begin with a refresher on earlier generative models like GANs and VAEs, and then progressively build toward training a basic diffusion model that can generate images from scratch.\n\nBy the end of this session, you will have:\n- Understood the theoretical principles behind diffusion models.\n- Implemented core components such as the noise scheduler and denoising network.\n- Trained a model to generate new samples.\n- Explored possible extensions, such as class-conditioning and improved sampling strategies.\n\n> Let's dive into the world of generative AI‚Äîone noisy step at a time!","metadata":{}},{"id":"7d137b70-9c5d-4701-9e66-9cbb19707b12","cell_type":"markdown","source":"## üìö Prerequisites\n<a id=\"prerequisites\"></a>\n\nBefore we begin building diffusion models, it‚Äôs important to ensure you're comfortable with the following concepts and tools. This will help you follow along and get the most out of the hands-on coding experience.\n\n### ‚úÖ Technical Knowledge\n- **Basic understanding of neural networks** and deep learning workflows.\n- Familiarity with **convolutional neural networks (CNNs)**.\n- Knowledge of **PyTorch** (especially `nn.Module`, data loaders, training loops).\n- Understanding of **loss functions** like MSE, and optimizers like Adam.\n\n### üß† Conceptual Foundations\n- What are **generative models**? (How they learn to model a data distribution)\n- Basic knowledge of **GANs (Generative Adversarial Networks)** and **VAEs (Variational Autoencoders)**.\n- Familiarity with **probability distributions** (especially Gaussian/Normal distributions).\n- Concept of **adding noise** to data and recovering it through learning.\n\nIf you're not familiar with some of these, don't worry‚Äîbrief refreshers will be provided along the way.\n\n> Let‚Äôs now take a quick look at traditional generative models (GANs and VAEs) before introducing diffusion models.\n","metadata":{}},{"id":"bf36ca06-1b86-4d15-b37b-9faf9a48e626","cell_type":"markdown","source":"## Building Diffusion Models\n### Code Implementation","metadata":{}},{"id":"88927972-9311-4fa5-bdf7-addbecd78bb1","cell_type":"markdown","source":"### Setting Up\n* Install and import required libraries\n* Define utility function to enable us visualize images\n* Define important variables","metadata":{}},{"id":"83f578d6","cell_type":"code","source":"# Install required Python packages using pip.\n# - lightning: for building and training PyTorch models easily\n# - torchmetrics: to calculate performance metrics like accuracy, FID, etc.\n# - torch-fidelity: for evaluating generative models (e.g., FID score)\n\n# Note: '--no-cache-dir' prevents using cached versions of the packages\n!pip install -q --no-cache-dir lightning torchmetrics torch-fidelity","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:00:11.729502Z","iopub.execute_input":"2025-05-26T22:00:11.729919Z","iopub.status.idle":"2025-05-26T22:00:17.146827Z","shell.execute_reply.started":"2025-05-26T22:00:11.729876Z","shell.execute_reply":"2025-05-26T22:00:17.144990Z"},"papermill":{"duration":203.854755,"end_time":"2025-05-26T16:54:29.138879","exception":false,"start_time":"2025-05-26T16:51:05.284124","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":2},{"id":"5758fada","cell_type":"code","source":"# Import Lightning and PyTorch Lightning utilities\nimport lightning as L\nfrom lightning.pytorch import Trainer, seed_everything\nfrom lightning.pytorch.callbacks import ModelCheckpoint  # For saving the best model\n\n# Import FID metric from TorchMetrics to evaluate generative models\nfrom torchmetrics.image.fid import FrechetInceptionDistance\n\n# Import a helper from Google Colab to display images with OpenCV\nfrom google.colab.patches import cv2_imshow\n\n# Import core PyTorch libraries\nimport torch\nimport torch.nn as nn  # For defining neural networks\nimport torch.optim as optim  # For optimization algorithms like Adam\nimport torch.utils.data as data  # For data loading and batching\nimport torch.nn.functional as F  # For functions like relu, softmax, etc.\n\n# Import torchvision tools for data transformation and augmentation\nimport torchvision.transforms as transforms\nfrom torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, Resize\n\n# PIL for image handling\nfrom PIL import Image\n\n# tqdm for showing progress bars\nfrom tqdm.auto import tqdm\n\n# Numpy and matplotlib for numerical operations and visualization\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Standard libraries\nimport os\nimport cv2\nimport random\nimport math\n\n# Ignore warning messages to keep the output clean\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Plot settings: inline display and aesthetic customization\n%matplotlib inline\nplt.rcParams['axes.facecolor'] = 'lightgray'  # Set background color of plots\nplt.rcParams['mathtext.fontset'] = 'cm'  # Use Computer Modern font for math\nplt.rcParams['font.family'] = 'STIXGeneral'  # Use STIX font for consistency","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:08:57.595704Z","iopub.execute_input":"2025-05-26T22:08:57.597103Z","iopub.status.idle":"2025-05-26T22:08:57.608660Z","shell.execute_reply.started":"2025-05-26T22:08:57.597057Z","shell.execute_reply":"2025-05-26T22:08:57.607489Z"},"papermill":{"duration":0.121395,"end_time":"2025-05-26T16:54:29.273322","exception":true,"start_time":"2025-05-26T16:54:29.151927","status":"failed"},"tags":[],"trusted":true},"outputs":[],"execution_count":32},{"id":"21f9c94b","cell_type":"code","source":"def show_tensor_image(image):\n    # Define a series of reverse transforms to convert a normalized tensor image\n    # back to a format that can be displayed as a proper image\n    reverse_transforms = Compose([\n        # Undo normalization from [-1, 1] to [0, 1]\n        Lambda(lambda t: (t + 1) / 2),\n\n        # Change shape from [C, H, W] to [H, W, C] for display\n        Lambda(lambda t: t.permute(1, 2, 0)),\n\n        # Scale pixel values from [0, 1] to [0, 255]\n        Lambda(lambda t: t * 255.),\n\n        # Move tensor to CPU, convert to NumPy array, and change dtype to uint8 for image display\n        Lambda(lambda t: t.cpu().numpy().astype(np.uint8)),\n\n        # Convert the NumPy array to a PIL Image\n        ToPILImage(),\n\n        # Resize the image to a fixed size (e.g., for consistent display)\n        Resize(IMAGE_SIZE),\n    ])\n\n    # If the input is a batch of images (4D tensor), take the first one only\n    if len(image.shape) == 4:\n        image = image[0, :, :, :]  # Select the first image in the batch\n\n    # Turn off axis labels and ticks\n    plt.axis('off')\n\n    # Apply reverse transforms and display the image using matplotlib\n    plt.imshow(reverse_transforms(image))\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:00:44.575709Z","iopub.execute_input":"2025-05-26T22:00:44.576321Z","iopub.status.idle":"2025-05-26T22:00:44.583188Z","shell.execute_reply.started":"2025-05-26T22:00:44.576291Z","shell.execute_reply":"2025-05-26T22:00:44.582258Z"}},"outputs":[],"execution_count":4},{"id":"8526b02d","cell_type":"code","source":"MAX_EPOCH = 30\nDIFFUSION_STEP = 1000\nBATCH_SIZE = 20\nLR = 3e-4\nCHECKPOINT_DIR = os.getcwd()\nIMAGE_SIZE = 48\nSCALE_DOWN = 2\nN_CHANNEL = 3 #INFO[FLAG]['n_channels']\nBETA_START = 1e-4\nBETA_END = 2e-2\nN_CLASSES = 2# len(INFO[FLAG]['label'])","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:00:44.585131Z","iopub.execute_input":"2025-05-26T22:00:44.585509Z","iopub.status.idle":"2025-05-26T22:00:44.591713Z","shell.execute_reply.started":"2025-05-26T22:00:44.585479Z","shell.execute_reply":"2025-05-26T22:00:44.590146Z"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1747755891211,"user":{"displayName":"Joshua Tettey Teye","userId":"03989818565665752308"},"user_tz":0},"id":"WCjIc6r__B8G","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":5},{"id":"6acbf373-7b9b-4c68-9e2c-118d60083fee","cell_type":"markdown","source":"## üóÇÔ∏è Dataset Preparation","metadata":{}},{"id":"7463d04b-3390-46b4-a26b-9b547f94d86f","cell_type":"markdown","source":"### üì• Adding the Medical MNIST Dataset to Your Kaggle Notebook\n\nBefore we begin training our model, we need to make sure the **Medical MNIST** dataset is available in our working environment.\n\nSince we're using **Kaggle Notebooks**, adding a dataset is straightforward and does not require any download or upload. Follow the steps below:\n\n#### ‚úÖ Steps to Add the Dataset from the Notebook\n\n1. **Open Your Kaggle Notebook**\n   Make sure you're inside the notebook you'll be working in.\n\n2. **Click on the ‚ÄúAdd Input‚Äù Panel**\n   - Look to the **right side** of your notebook interface.\n   - You‚Äôll see a sidebar labeled **‚ÄúInput,‚Äù ‚ÄúOutput,‚Äù ‚ÄúTable of Contents,‚Äù etc.**\n   - Click on the **‚Äú+ Add Input‚Äù** button under the **‚ÄúInput‚Äù** tab.\n\n3. **Search for the Dataset**\n   - In the search box that appears, type:  \n     ```\n     medical mnist\n     ```\n   - Locate the dataset titled **‚ÄúMedical MNIST‚Äù** (uploaded by `Larxel`).\n\n4. **Attach the Dataset**\n   - Click the **‚ÄúAdd‚Äù (+)** button next to the dataset name.\n   - This will attach the dataset to your notebook under the `/kaggle/input/` directory.\n","metadata":{}},{"id":"90e37948","cell_type":"markdown","source":"### Processing the Dataset\nIn this section, we‚Äôll walk through how to **prepare a custom dataset** for training a conditional diffusion model. The goal is to allow our model to learn to generate images from specific classes ‚Äî *Chest* vs *Hand* X-ray images.\n\n1. **Image Preprocessing**  \n   We'll define a transformation pipeline that resizes, normalizes, and converts the images into tensors. This ensures that all images are in the same format and scale before being fed into our model.\n\n2. **Label Formatting (One-Hot Encoding)**  \n   Since we‚Äôre working with two classes and using a class-conditioned diffusion model, we‚Äôll convert each class label into a **one-hot encoded tensor**. This allows the model to receive class information in a structured and consistent format during training.\n\n3. **Custom Dataset Class**  \n   To have more control over which classes are loaded from a dataset, we‚Äôll build a **custom dataset class** by extending `torchvision.datasets.DatasetFolder`. This will allow us to:\n   - Filter and load only specific subfolders (i.e., only the desired classes),\n   - Apply our transformations,\n   - Return both the image and its corresponding one-hot encoded label.\n","metadata":{"id":"PrhyQrbmdFCQ","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"id":"0881fae5","cell_type":"code","source":"# Define a transformation pipeline to preprocess images before feeding them into a model\nimage_transform = Compose([\n    # Resize the image to a fixed size (IMAGE_SIZE x IMAGE_SIZE)\n    Resize((IMAGE_SIZE, IMAGE_SIZE)),\n\n    # Convert the image from PIL or numpy format to a PyTorch tensor\n    # This also scales pixel values from [0, 255] to [0.0, 1.0]\n    ToTensor(),\n\n    # Normalize the tensor values from [0.0, 1.0] to [-1.0, 1.0]\n    Lambda(lambda x: (x * 2) - 1),\n])\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:00:44.592866Z","iopub.execute_input":"2025-05-26T22:00:44.593144Z","iopub.status.idle":"2025-05-26T22:00:44.599148Z","shell.execute_reply.started":"2025-05-26T22:00:44.593121Z","shell.execute_reply":"2025-05-26T22:00:44.597841Z"}},"outputs":[],"execution_count":6},{"id":"c91ce3cc","cell_type":"code","source":"# This function converts a class label into a one-hot encoded tensor with 2 classes\ndef one_hot_encode(label):\n    # Convert the label (e.g., 0 or 1) to a PyTorch tensor\n    # Then apply one-hot encoding with 2 classes (e.g., [1, 0] for class 0, [0, 1] for class 1)\n    return F.one_hot(torch.tensor(label), num_classes=2).float()\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:00:44.600456Z","iopub.execute_input":"2025-05-26T22:00:44.600838Z","iopub.status.idle":"2025-05-26T22:00:44.605976Z","shell.execute_reply.started":"2025-05-26T22:00:44.600805Z","shell.execute_reply":"2025-05-26T22:00:44.604947Z"}},"outputs":[],"execution_count":7},{"id":"343de6cc","cell_type":"code","source":"from torchvision.datasets import DatasetFolder\nfrom torchvision.datasets.folder import default_loader\nfrom typing import List, Tuple, Dict\nimport os\n\n# A custom dataset class that extends torchvision's DatasetFolder\n# It filters and loads only images from specified subfolders (i.e., desired classes)\nclass CustomImageFolder(DatasetFolder):\n    def __init__(\n        self,\n        root: str,                          # Root directory containing class-named folders\n        desired_classes: List[str],         # List of class folder names to include\n        transform=None,                     # Optional image transformations\n        target_transform=None,              # Optional label transformations\n        loader=default_loader,              # Function to load an image file\n        is_valid_file=None                  # Optional custom file filter\n    ):\n        self.desired_classes = desired_classes  # Store desired class names\n        # Call the parent DatasetFolder constructor with appropriate settings\n        super().__init__(\n            root,\n            loader,\n            extensions=('jpg', 'jpeg', 'png'),  # Allowed image formats\n            transform=transform,\n            target_transform=target_transform,\n            is_valid_file=is_valid_file\n        )\n\n    def find_classes(self, directory: str) -> Tuple[List[str], Dict[str, int]]:\n        \"\"\"\n        Override the default class discovery to include only desired classes.\n\n        Args:\n            directory (str): Path to the root dataset directory.\n\n        Returns:\n            classes (List[str]): List of selected class folder names.\n            class_to_idx (Dict[str, int]): Mapping from class names to indices.\n        \"\"\"\n        # Scan the dataset directory and list all subfolders\n        all_classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n\n        # Filter only those that match the desired_classes list\n        classes = [cls for cls in all_classes if cls in self.desired_classes]\n\n        # Raise an error if none of the desired classes were found\n        if not classes:\n            raise FileNotFoundError(\n                f\"No matching class folders found in {directory}. \"\n                f\"Available: {all_classes}\"\n            )\n\n        # Create a dictionary mapping class names to integer indices\n        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n\n        return classes, class_to_idx\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:00:44.607080Z","iopub.execute_input":"2025-05-26T22:00:44.607517Z","iopub.status.idle":"2025-05-26T22:00:44.616698Z","shell.execute_reply.started":"2025-05-26T22:00:44.607483Z","shell.execute_reply":"2025-05-26T22:00:44.615653Z"}},"outputs":[],"execution_count":8},{"id":"99aad484","cell_type":"code","source":"dataset = CustomImageFolder(\n    root=\"/kaggle/input/medical-mnist\",        # Path to the root directory of the dataset.\n                                                # This directory should contain subfolders named after class labels (e.g., \"CXR\", \"Hand\", etc.)\n\n    desired_classes=[\"CXR\", \"Hand\"],            # Only include images from the \"CXR\" and \"Hand\" subfolders (i.e., filter by desired classes).\n\n    transform=image_transform,                  # Apply preprocessing to each image:\n                                                # - Resize to IMAGE_SIZE x IMAGE_SIZE\n                                                # - Convert to tensor\n                                                # - Normalize from [0, 1] to [-1, 1]\n\n    target_transform=one_hot_encode             # Apply one-hot encoding to the class label (e.g., 0 -> [1, 0], 1 -> [0, 1])\n)\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:01:41.017103Z","iopub.execute_input":"2025-05-26T22:01:41.017500Z","iopub.status.idle":"2025-05-26T22:02:23.967006Z","shell.execute_reply.started":"2025-05-26T22:01:41.017473Z","shell.execute_reply":"2025-05-26T22:02:23.965791Z"}},"outputs":[],"execution_count":10},{"id":"43cab0a2","cell_type":"code","source":"# Print the list of class folder names included in the dataset\n# This will only include the classes specified in 'desired_classes' (e.g., ['CXR', 'Hand'])\nprint(dataset.classes)\n\n# Print the mapping of class names to numerical indices\n# This dictionary is used internally to assign numeric labels to each class\nprint(dataset.class_to_idx)\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:02:23.969009Z","iopub.execute_input":"2025-05-26T22:02:23.969323Z","iopub.status.idle":"2025-05-26T22:02:23.974546Z","shell.execute_reply.started":"2025-05-26T22:02:23.969301Z","shell.execute_reply":"2025-05-26T22:02:23.973614Z"}},"outputs":[{"name":"stdout","text":"['CXR', 'Hand']\n{'CXR': 0, 'Hand': 1}\n","output_type":"stream"}],"execution_count":11},{"id":"21441940","cell_type":"code","source":"# Count the number of samples per class in the dataset\n# dataset.targets is a list of class indices for each image (e.g., [0, 0, 1, 0, 1, 1, ...])\nclass_counts = torch.bincount(torch.tensor(dataset.targets))\n\n# Loop through each class and print its name and corresponding image count\nfor i, count in enumerate(class_counts):\n    # dataset.classes[i] gets the class name (e.g., 'CXR' or 'Hand')\n    # count.item() gets the number of images for that class\n    print(f\"Class: {dataset.classes[i]}, Count: {count.item()}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:02:23.975404Z","iopub.execute_input":"2025-05-26T22:02:23.975642Z","iopub.status.idle":"2025-05-26T22:02:23.990714Z","shell.execute_reply.started":"2025-05-26T22:02:23.975624Z","shell.execute_reply":"2025-05-26T22:02:23.989734Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Class: CXR, Count: 10000\nClass: Hand, Count: 10000\n","output_type":"stream"}],"execution_count":12},{"id":"3123f830","cell_type":"code","source":"# Step 2: Define the number of samples for validation and test sets\nval_size = 4        # Number of samples to reserve for validation\ntest_size = 4       # Number of samples to reserve for testing\n\n# Compute the number of samples for the training set\n# Total = len(dataset), so training = total - val - test\ntrain_size = len(dataset) - val_size - test_size\n\n# Step 3: Randomly split the dataset into training, validation, and test sets\nTrainDataset, ValDataset, TestDataset = data.random_split(\n    dataset,                         # The full dataset to split\n    [train_size, val_size, test_size],  # List of sizes for each split\n)","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:02:23.993154Z","iopub.execute_input":"2025-05-26T22:02:23.993494Z","iopub.status.idle":"2025-05-26T22:02:24.000863Z","shell.execute_reply.started":"2025-05-26T22:02:23.993470Z","shell.execute_reply":"2025-05-26T22:02:23.999586Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":13},{"id":"b34953b6","cell_type":"code","source":"# Get one sample (image and label) from the training dataset\nimages = TrainDataset[0]  # This returns a tuple: (image_tensor, label_tensor)\n\n# Print the shape of the image tensor\nprint(images[0].shape)    # Should be something like torch.Size([3, H, W])\n\n# Extract the label from the tuple\nlabel = images[1]         # Can be a one-hot encoded tensor or class index depending on your target_transform\n\n# Print the type of the label (e.g., <class 'torch.Tensor'>)\nprint(type(label))\n\n# Print the actual label value (e.g., tensor([1., 0.]))\nprint(\"label\", label)\n\n# Another way to explicitly show the image shape\nprint(f\"Image shape: {images[0].shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:02:24.001893Z","iopub.execute_input":"2025-05-26T22:02:24.002148Z","iopub.status.idle":"2025-05-26T22:02:24.121123Z","shell.execute_reply.started":"2025-05-26T22:02:24.002131Z","shell.execute_reply":"2025-05-26T22:02:24.119898Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([3, 48, 48])\n<class 'torch.Tensor'>\nlabel tensor([0., 1.])\nImage shape: torch.Size([3, 48, 48])\n","output_type":"stream"}],"execution_count":14},{"id":"8e75f75b","cell_type":"code","source":"# Initialize an index counter\nindex = 0\n\n# Iterate through all samples (image, label) in the training dataset\nfor image, label in TrainDataset:\n    \n    # Check if the label matches [1., 0.] ‚Äî i.e., class 0 in one-hot encoding\n    # (label == torch.tensor([1., 0.])) returns a boolean tensor, so sum(...) counts how many elements match\n    if sum(label == torch.tensor([1., 0.])) > 1:\n        \n        # If it's a match, increment the counter\n        index += 1\n\n        # Display the image using your custom image display function\n        show_tensor_image(image)\n\n        # Break after showing more than 5 matching images\n        if index > 5:\n            break","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:02:24.122210Z","iopub.execute_input":"2025-05-26T22:02:24.122598Z","iopub.status.idle":"2025-05-26T22:02:24.466976Z","shell.execute_reply.started":"2025-05-26T22:02:24.122567Z","shell.execute_reply":"2025-05-26T22:02:24.466050Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":15},{"id":"1cd39bf8","cell_type":"markdown","source":"## Coding the Diffusion Model\n\nIn this section, we‚Äôll begin building our **Diffusion Model** ‚Äî a powerful generative model used for creating high-quality synthetic images. We'll break it down into understandable, modular components so that each part makes sense and can be reused or extended later.\n\n\n### üîß Step-by-Step Breakdown\n\nWe'll proceed in the following order:\n\n1. **üìä Utility Function (AverageMeter)**\n   - We'll define a simple utility class to keep track of metrics like loss during training.\n\n\n2. **üå´Ô∏è Noise Scheduler**\n   - The forward (diffusion) process starts here.\n   - This component defines how noise is added to clean images over time (timesteps), simulating the degradation process.\n\n\n3. **üß± Core U-Net Architecture**\n   - Our model will follow a **U-Net**-like structure, which is commonly used in denoising and image segmentation tasks.\n   - We'll implement it **block by block**, building from the ground up:\n     - **Self-Attention Block:** Helps the model learn long-range dependencies.\n     - **Double Convolution Block:** Applies two successive convolutional layers to capture features.\n     - **Downsampling Block:** Reduces spatial resolution and increases feature abstraction.\n     - **Upsampling Block:** Restores spatial resolution, guided by features from earlier layers.\n     - **Positional Embeddings:** Encodes timestep and context information into the model.\n\n\n4. **üß© Assembling the U-Net**\n   - All components from step 3 are brought together to construct the full U-Net.\n\n5. **üåÄ The Diffusion Model**\n   - Here we combine:\n     - The **Noise Scheduler** from step 2\n     - The **U-Net** model from step 4\n   - This results in a complete diffusion system that learns how to **reverse the noise process**, gradually transforming pure noise into meaningful images conditioned on class labels or time.\n\n","metadata":{"id":"M9bvhMihdFNH","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"id":"9c332fe5","cell_type":"code","source":"class AvgMeter(object):\n    def __init__(self, num=40):\n        \"\"\"\n        Initialize the average meter.\n\n        Args:\n            num (int): Number of recent values to keep for computing the average.\n        \"\"\"\n        self.num = num  # Limit to the most recent `num` values\n        self.reset()    # Initialize/reset the scores list\n\n    def reset(self):\n        \"\"\"\n        Clear the stored scores.\n        \"\"\"\n        self.scores = []\n\n    def update(self, val):\n        \"\"\"\n        Add a new value to the score list.\n\n        Args:\n            val (float or tensor): The new value to track.\n        \"\"\"\n        self.scores.append(val)\n\n    def show(self):\n        \"\"\"\n        Compute and return the average of the most recent `num` values.\n\n        Returns:\n            torch.Tensor: The mean of the recent values as a torch scalar.\n        \"\"\"\n        # Only use the last `num` values for averaging\n        out = torch.mean(\n            torch.stack(\n                self.scores[np.maximum(len(self.scores) - self.num, 0):]  # Get last `num` elements\n            )\n        )\n        return out\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:02:24.468118Z","iopub.execute_input":"2025-05-26T22:02:24.468481Z","iopub.status.idle":"2025-05-26T22:02:24.474886Z","shell.execute_reply.started":"2025-05-26T22:02:24.468458Z","shell.execute_reply":"2025-05-26T22:02:24.473906Z"}},"outputs":[],"execution_count":16},{"id":"07624656","cell_type":"markdown","source":"### Noise Scheduler","metadata":{"id":"OUP90Trjij2r","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"id":"17390426","cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass NoiseScheduler(nn.Module):\n    def __init__(\n        self,\n        T=DIFFUSION_STEP,          # Total number of diffusion steps (e.g., 1000)\n        beta_start=BETA_START,     # Starting value of beta (e.g., 1e-4)\n        beta_end=BETA_END          # Ending value of beta (e.g., 0.02)\n    ):\n        super().__init__()\n\n        # Number of timesteps in the diffusion process\n        self.T = T\n\n        # Linearly spaced beta schedule: controls the amount of noise added at each step\n        self.beta = torch.linspace(beta_start, beta_end, T).to(\n            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        )\n\n        # alpha = 1 - beta, which represents the amount of original signal preserved at each step\n        self.alpha = 1. - self.beta\n\n        # alpha_hat = cumulative product of alpha over time\n        # This represents the overall preservation of the original signal up to time t\n        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n\n    def forward(self, x, t):\n        \"\"\"\n        Add noise to input `x` based on timestep `t`.\n\n        Args:\n            x (Tensor): Original clean image tensor [B, C, H, W]\n            t (Tensor): Timestep tensor [B] containing values in [0, T)\n\n        Returns:\n            x_noisy (Tensor): Noisy version of the input image\n            noise (Tensor): The random noise that was added\n        \"\"\"\n        # Reshape alpha_hat[t] to [B, 1, 1, 1] to match image dimensions\n        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n        sqrt_one_minus_alpha_hat = torch.sqrt(\n            1 - self.alpha_hat[t]\n        ).unsqueeze(1).unsqueeze(1).unsqueeze(1)\n\n        # Generate standard Gaussian noise of the same shape as x\n        noise = torch.randn_like(x)\n\n        # Create the noisy image using the reparameterization formula:\n        # x_noisy = sqrt(alpha_hat) * x + sqrt(1 - alpha_hat) * noise\n        x_noisy = sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * noise\n\n        return x_noisy, noise\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:02:24.475849Z","iopub.execute_input":"2025-05-26T22:02:24.476101Z","iopub.status.idle":"2025-05-26T22:02:24.483934Z","shell.execute_reply.started":"2025-05-26T22:02:24.476081Z","shell.execute_reply":"2025-05-26T22:02:24.482811Z"}},"outputs":[],"execution_count":17},{"id":"da7b7464","cell_type":"code","source":"FORWARD = NoiseScheduler","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:02:24.484950Z","iopub.execute_input":"2025-05-26T22:02:24.485260Z","iopub.status.idle":"2025-05-26T22:02:24.489898Z","shell.execute_reply.started":"2025-05-26T22:02:24.485238Z","shell.execute_reply":"2025-05-26T22:02:24.488852Z"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1747758137302,"user":{"displayName":"Joshua Tettey Teye","userId":"03989818565665752308"},"user_tz":0},"id":"C1eyVzjbmpH_","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":18},{"id":"4adf5857","cell_type":"markdown","source":"### **U-Net**","metadata":{"id":"uWcPMvglioOY","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"id":"6baf149f","cell_type":"code","source":"class SelfAttention(nn.Module):\n    def __init__(self, channels, size):\n        super(SelfAttention, self).__init__()\n        \n        self.channels = channels  # Number of channels in the input (e.g., feature maps)\n        self.size = size          # Spatial size (height/width) of the feature map\n\n        # Multi-Head Attention: operates on flattened spatial features\n        self.mha = nn.MultiheadAttention(\n            embed_dim=channels,     # Each token has 'channels' features\n            num_heads=4,            # Number of attention heads\n            batch_first=True        # Input/output shape: (B, N, C) instead of default (N, B, C)\n        )\n\n        # LayerNorm before attention\n        self.ln = nn.LayerNorm([channels])\n\n        # Feed-forward block after attention\n        self.ff_self = nn.Sequential(\n            nn.LayerNorm([channels]),\n            nn.Linear(channels, channels),\n            nn.GELU(),\n            nn.Linear(channels, channels),\n        )\n\n    def forward(self, x):\n        # Input x: [B, C, H, W]\n        \n        # Reshape to [B, N, C] where N = H*W (flatten spatial dims), suitable for attention\n        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)  # => [B, N, C]\n\n        # Apply LayerNorm before attention (standard transformer practice)\n        x_ln = self.ln(x)\n\n        # Apply multi-head self-attention: query, key, value are all x_ln\n        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n\n        # Residual connection: Add input x to attention output\n        attention_value = attention_value + x\n\n        # Feed-forward network with residual connection\n        attention_value = self.ff_self(attention_value) + attention_value\n\n        # Reshape back to original spatial dimensions: [B, C, H, W]\n        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:02:24.493151Z","iopub.execute_input":"2025-05-26T22:02:24.493498Z","iopub.status.idle":"2025-05-26T22:02:24.501608Z","shell.execute_reply.started":"2025-05-26T22:02:24.493471Z","shell.execute_reply":"2025-05-26T22:02:24.500485Z"}},"outputs":[],"execution_count":19},{"id":"c311caed","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DoubleConv(nn.Module):\n    def __init__(\n        self, in_channels, out_channels, mid_channels=None, residual=False,\n    ):\n        super().__init__()\n        self.residual = residual  # Whether to use residual connection\n\n        # If no mid_channels is provided, set it equal to out_channels\n        if not mid_channels:\n            mid_channels = out_channels\n\n        # Define a sequence of two convolutional layers with normalization and GELU activation\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(\n                in_channels, mid_channels, kernel_size=3, padding=1, bias=False\n            ),                              # First Conv: in_channels ‚Üí mid_channels\n            nn.GroupNorm(1, mid_channels),  # Group normalization with 1 group (acts like LayerNorm)\n            nn.GELU(),                      # GELU activation (smoother than ReLU)\n\n            nn.Conv2d(\n                mid_channels, out_channels, kernel_size=3, padding=1, bias=False\n            ),                              # Second Conv: mid_channels ‚Üí out_channels\n            nn.GroupNorm(1, out_channels),  # Another GroupNorm\n        )\n\n    def forward(self, x):\n        # If residual is enabled, add input `x` to the convolutional output before GELU\n        if self.residual:\n            return F.gelu(x + self.double_conv(x))\n        else:\n            return self.double_conv(x)\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:02:24.503043Z","iopub.execute_input":"2025-05-26T22:02:24.503417Z","iopub.status.idle":"2025-05-26T22:02:24.510907Z","shell.execute_reply.started":"2025-05-26T22:02:24.503386Z","shell.execute_reply":"2025-05-26T22:02:24.510029Z"}},"outputs":[],"execution_count":20},{"id":"7d649b00","cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass Down(nn.Module):\n    def __init__(self, in_channels, out_channels, time_emb_dim, context_emb_dim=N_CLASSES):\n        super().__init__()\n\n        # Downsampling block with:\n        # - MaxPooling to reduce spatial resolution by 2\n        # - Two DoubleConv layers (first with residual)\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),  # Downsample by a factor of 2\n            DoubleConv(in_channels, in_channels, residual=True),\n            DoubleConv(in_channels, out_channels),\n        )\n\n        # Linear layer to embed the time step into a feature map with 'out_channels' dimensions\n        self.time_emb_layer = nn.Sequential(\n            nn.SiLU(),  # Activation function (Sigmoid-weighted ReLU)\n            nn.Linear(time_emb_dim, out_channels),  # Time embedding projection\n        )\n\n        # Linear layer to embed the class/context label into the same shape\n        self.context_emb_layer = nn.Sequential(\n            nn.SiLU(),  # Activation function\n            nn.Linear(context_emb_dim, out_channels),  # Context embedding projection\n        )\n\n    def forward(self, x, y, t):\n        \"\"\"\n        Args:\n            x: Input feature map (B, C, H, W)\n            y: Context embedding input (e.g., class label vector) ‚Üí shape (B, context_emb_dim)\n            t: Time embedding input (e.g., timestep vector) ‚Üí shape (B, time_emb_dim)\n\n        Returns:\n            A feature map with both time and context conditioning applied.\n        \"\"\"\n\n        # Apply downsampling and convolutional operations\n        x = self.maxpool_conv(x)  # Output shape: (B, out_channels, H/2, W/2)\n\n        # Process time embedding and reshape to broadcast over spatial dimensions\n        t_emb = self.time_emb_layer(t)[:, :, None, None].repeat(\n            1, 1, x.shape[-2], x.shape[-1]\n        )\n\n        # Process context (e.g., class embedding) and reshape to broadcast\n        y_emb = self.context_emb_layer(y)[:, :, None, None].repeat(\n            1, 1, x.shape[-2], x.shape[-1]\n        )\n\n        # Apply conditioning: modulate x with y_emb (multiplicative) and t_emb (additive)\n        return (y_emb * x) + t_emb\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:02:24.511896Z","iopub.execute_input":"2025-05-26T22:02:24.512161Z","iopub.status.idle":"2025-05-26T22:02:24.520961Z","shell.execute_reply.started":"2025-05-26T22:02:24.512140Z","shell.execute_reply":"2025-05-26T22:02:24.519798Z"}},"outputs":[],"execution_count":21},{"id":"b5eb2ca6","cell_type":"code","source":"class Up(nn.Module):\n    def __init__(self, in_channels, out_channels, time_emb_dim, context_emb_dim=N_CLASSES):\n        super().__init__()\n\n        # Upsampling operation to increase spatial resolution by a factor of 2\n        self.up = nn.Upsample(\n            scale_factor=2,\n            mode=\"bilinear\",        # Bilinear interpolation for smoother upscaling\n            align_corners=True,\n        )\n\n        # Double convolution block after upsampling and skip connection concatenation\n        self.conv = nn.Sequential(\n            DoubleConv(in_channels, in_channels, residual=True),       # Residual double conv\n            DoubleConv(in_channels, out_channels, in_channels // 2),   # Standard double conv\n        )\n\n        # Project time embedding to match the number of output channels\n        self.time_emb_layer = nn.Sequential(\n            nn.SiLU(),                      # Activation function\n            nn.Linear(time_emb_dim, out_channels),  # Map time embedding ‚Üí channel space\n        )\n\n        # Project context (e.g. class label) embedding to output channels\n        self.context_emb_layer = nn.Sequential(\n            nn.SiLU(),                             # Activation\n            nn.Linear(context_emb_dim, out_channels),  # Map context ‚Üí channel space\n        )\n\n    def forward(self, x, skip_x, y, t):\n        \"\"\"\n        Forward pass of the Up block.\n\n        Args:\n            x (Tensor): Input feature map from the previous layer [B, C, H, W]\n            skip_x (Tensor): Feature map from the encoder for skip connection [B, C_skip, H, W]\n            y (Tensor): Context embedding [B, context_emb_dim]\n            t (Tensor): Time embedding [B, time_emb_dim]\n\n        Returns:\n            Tensor: Output feature map with conditioning applied\n        \"\"\"\n\n        # Upsample the input\n        x = self.up(x)\n\n        # Concatenate the skip connection from encoder (along channel dimension)\n        x = torch.cat([skip_x, x], dim=1)  # Shape: [B, C_skip + C, H, W]\n\n        # Apply convolutional layers\n        x = self.conv(x)\n\n        # Expand time embedding to spatial dimensions and match channels\n        t_emb = self.time_emb_layer(t)[:, :, None, None].repeat(\n            1, 1, x.shape[-2], x.shape[-1]\n        )\n\n        # Expand context embedding to spatial dimensions\n        y_emb = self.context_emb_layer(y)[:, :, None, None].repeat(\n            1, 1, x.shape[-2], x.shape[-1]\n        )\n\n        # Apply context as multiplicative modulation, and time as additive bias\n        return (y_emb * x) + t_emb\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:02:24.522229Z","iopub.execute_input":"2025-05-26T22:02:24.522610Z","iopub.status.idle":"2025-05-26T22:02:24.532328Z","shell.execute_reply.started":"2025-05-26T22:02:24.522579Z","shell.execute_reply":"2025-05-26T22:02:24.531050Z"}},"outputs":[],"execution_count":22},{"id":"c98d46c3","cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass PositionalEmbedding(nn.Module):\n    def __init__(\n        self,\n        channels,\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    ):\n        super().__init__()\n        self.channels = channels  # Dimensionality of the embedding vector\n        self.device = device      # Device to place tensors on (e.g., \"cuda\" or \"cpu\")\n\n    def forward(self, time):\n        \"\"\"\n        Converts a scalar time tensor into a sinusoidal positional embedding.\n\n        Args:\n            time (Tensor): A tensor of shape [B, 1] or [B] representing timesteps.\n\n        Returns:\n            Tensor: A positional embedding of shape [B, channels]\n        \"\"\"\n\n        # Compute inverse frequencies for sinusoidal functions:\n        # inv_freq = [1/10000^(2i/channels) for i in (0, 2, 4, ...)]\n        inv_freq = 1.0 / (\n            10000 ** (\n                torch.arange(0, self.channels, 2, device=self.device).float() / self.channels\n            )\n        )\n\n        # Make sure time has shape [B, 1] for broadcasting\n        time = time.unsqueeze(1) if time.dim() == 1 else time\n\n        # Repeat time values to match the number of frequencies\n        # Apply sin to even indices, cos to odd indices\n        pos_enc_a = torch.sin(time * inv_freq)  # shape: [B, channels//2]\n        pos_enc_b = torch.cos(time * inv_freq)  # shape: [B, channels//2]\n\n        # Concatenate sin and cos components to get the full positional embedding\n        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)  # shape: [B, channels]\n\n        return pos_enc\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:02:24.533414Z","iopub.execute_input":"2025-05-26T22:02:24.533689Z","iopub.status.idle":"2025-05-26T22:02:24.541406Z","shell.execute_reply.started":"2025-05-26T22:02:24.533670Z","shell.execute_reply":"2025-05-26T22:02:24.540433Z"}},"outputs":[],"execution_count":23},{"id":"533f3453","cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(\n        self,\n        c_in=N_CHANNEL,                          # Number of input channels (e.g., 3 for RGB)\n        c_out=N_CHANNEL,                         # Number of output channels\n        time_dim=256 // SCALE_DOWN,              # Dimensionality of time embeddings\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n    ):\n        super().__init__()\n\n        # Initial convolutional block\n        self.input_conv = DoubleConv(c_in, 64 // SCALE_DOWN)\n\n        # Encoder (Downsampling path)\n        self.down1 = Down(64 // SCALE_DOWN, 128 // SCALE_DOWN, time_dim)\n        self.sa1 = SelfAttention(128 // SCALE_DOWN, IMAGE_SIZE // 2)  # Self-attention after down1\n\n        self.down2 = Down(128 // SCALE_DOWN, 256 // SCALE_DOWN, time_dim)\n        self.sa2 = SelfAttention(256 // SCALE_DOWN, IMAGE_SIZE // 4)\n\n        self.down3 = Down(256 // SCALE_DOWN, 256 // SCALE_DOWN, time_dim)\n        self.sa3 = SelfAttention(256 // SCALE_DOWN, IMAGE_SIZE // 8)\n\n        # Bridge (bottleneck)\n        self.bridge1 = DoubleConv(256 // SCALE_DOWN, 512 // SCALE_DOWN)\n        self.bridge2 = DoubleConv(512 // SCALE_DOWN, 512 // SCALE_DOWN)\n        self.bridge3 = DoubleConv(512 // SCALE_DOWN, 256 // SCALE_DOWN)\n\n        # Decoder (Upsampling path)\n        self.up1 = Up(512 // SCALE_DOWN, 128 // SCALE_DOWN, time_dim)\n        self.sa4 = SelfAttention(128 // SCALE_DOWN, IMAGE_SIZE // 4)\n\n        self.up2 = Up(256 // SCALE_DOWN, 64 // SCALE_DOWN, time_dim)\n        self.sa5 = SelfAttention(64 // SCALE_DOWN, IMAGE_SIZE // 2)\n\n        self.up3 = Up(128 // SCALE_DOWN, 64 // SCALE_DOWN, time_dim)\n        self.sa6 = SelfAttention(64 // SCALE_DOWN, IMAGE_SIZE)\n\n        # Final convolution to produce output image\n        self.out_conv = nn.Conv2d(64 // SCALE_DOWN, c_out, kernel_size=1)\n\n        # Time positional encoding (used to condition the model on timestep `t`)\n        self.pos_encoding = PositionalEmbedding(time_dim, device)\n\n    def forward(self, x, y, t):\n        \"\"\"\n        Forward pass of the conditional U-Net.\n\n        Args:\n            x: Input image tensor (B, C, H, W)\n            y: Class embedding or one-hot vector (B, N_CLASSES)\n            t: Time step tensor (B)\n\n        Returns:\n            Tensor: Output image tensor (B, C_out, H, W)\n        \"\"\"\n        # Expand and encode the time embedding\n        t = t.unsqueeze(-1).type(torch.float)     # Ensure shape [B, 1]\n        t = self.pos_encoding(t)                  # Get time embedding [B, time_dim]\n\n        # Downsampling path\n        x1 = self.input_conv(x)                   # Initial conv\n        x2 = self.down1(x1, y, t)\n        x2 = self.sa1(x2)\n\n        x3 = self.down2(x2, y, t)\n        x3 = self.sa2(x3)\n\n        x4 = self.down3(x3, y, t)\n        x4 = self.sa3(x4)\n\n        # Bottleneck (bridge)\n        x4 = self.bridge1(x4)\n        x4 = self.bridge2(x4)\n        x4 = self.bridge3(x4)\n\n        # Upsampling path with skip connections\n        x = self.up1(x4, x3, y, t)\n        x = self.sa4(x)\n\n        x = self.up2(x, x2, y, t)\n        x = self.sa5(x)\n\n        x = self.up3(x, x1, y, t)\n        x = self.sa6(x)\n\n        # Final output layer\n        output = self.out_conv(x)\n\n        return output\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:02:24.542323Z","iopub.execute_input":"2025-05-26T22:02:24.542686Z","iopub.status.idle":"2025-05-26T22:02:24.554949Z","shell.execute_reply.started":"2025-05-26T22:02:24.542655Z","shell.execute_reply":"2025-05-26T22:02:24.553950Z"}},"outputs":[],"execution_count":24},{"id":"810a6a4e","cell_type":"code","source":"BACKWARD = UNet","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:02:24.555891Z","iopub.execute_input":"2025-05-26T22:02:24.556114Z","iopub.status.idle":"2025-05-26T22:02:24.560028Z","shell.execute_reply.started":"2025-05-26T22:02:24.556097Z","shell.execute_reply":"2025-05-26T22:02:24.559158Z"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1747758141286,"user":{"displayName":"Joshua Tettey Teye","userId":"03989818565665752308"},"user_tz":0},"id":"f8Y-sm8FmlrI","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":25},{"id":"6431ecad","cell_type":"markdown","source":"### **Wrapper**","metadata":{"id":"tn_nP8Uqmi6b","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"id":"65df07db","cell_type":"code","source":"class DiffusionModel(L.LightningModule):\n    def __init__(self, forward_model, backward_model, batch_size, lr, max_epoch):\n        super().__init__()\n        \n        # Diffusion models\n        self.forward_model = forward_model  # Noise scheduler\n        self.backward_model = backward_model  # Denoising UNet\n\n        # Training parameters\n        self.batch_size = batch_size\n        self.lr = lr\n        self.max_epoch = max_epoch\n\n        # Manual optimization\n        self.automatic_optimization = False\n\n        # Metrics\n        self.model_loss = []\n        self.val_fid = []\n\n        self.model_loss_recorder = AvgMeter()\n        self.val_fid_recorder = AvgMeter()\n\n        self._device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self._T = self.forward_model.T\n\n        # FID computation\n        self.fid = FrechetInceptionDistance(\n            feature=64,\n            input_img_size=(N_CHANNEL, IMAGE_SIZE, IMAGE_SIZE),\n            normalize=True\n        )\n\n    def forward(self, x=None, y=None, t=None):\n        # Training: return MSE loss between predicted and true noise\n        if self.training:\n            x_noisy, noise = self.forward_model(x, t)\n            noise_pred = self.backward_model(x_noisy, y, t)\n            return F.mse_loss(noise, noise_pred)\n        else:\n            # Sampling mode during validation or inference\n            return self.sample(progress=False, verbose=True)\n\n    def sample(self, n=1, context=None, progress=False, verbose=False, n_progress=5):\n        self.backward_model.eval()\n\n        # If no context is provided, randomly generate one-hot class context\n        context = F.one_hot(torch.tensor(np.random.randint(0, 2), dtype=torch.int64),\n                            num_classes=N_CLASSES).float() if context is None else context\n\n        progress_image = [] if progress else None\n\n        with torch.no_grad():\n            x = torch.randn((n, N_CHANNEL, IMAGE_SIZE, IMAGE_SIZE)).to(self.device)\n\n            if progress:\n                progress_image.append(x.detach().cpu())\n\n            iteration = tqdm(reversed(range(0, self._T)), position=0) if verbose else reversed(range(0, self._T))\n\n            for i in iteration:\n                t = (torch.ones(n) * i).long().to(self.device)\n\n                noise_pred = self.backward_model(x, context, t)\n\n                alpha = self.forward_model.alpha[t][:, None, None, None]\n                alpha_hat = self.forward_model.alpha_hat[t][:, None, None, None]\n                beta = self.forward_model.beta[t][:, None, None, None]\n\n                noise = torch.randn_like(x) if i else torch.zeros_like(x)\n\n                # Reverse process (denoising step)\n                x = (\n                    1 / torch.sqrt(alpha) *\n                    (x - ((1 - alpha) / torch.sqrt(1 - alpha_hat)) * noise_pred) +\n                    torch.sqrt(beta) * noise\n                )\n                x = torch.clamp(x, -1.0, 1.0)\n\n                if progress and i % (self._T // n_progress) == 0:\n                    progress_image.append(x.detach().cpu())\n\n            if progress:\n                progress_image.pop(n_progress // 2)\n                return progress_image\n\n        return x\n\n    def on_train_epoch_start(self):\n        self.fid.reset()\n\n    def training_step(self, batch, batch_nb):\n        x, y = batch\n\n        self.fid.update((x + 1.0) / 2.0, real=True)\n\n        t = torch.randint(0, self._T, (self.batch_size,), device=self._device).long()\n        loss = self(x, y, t)\n\n        opt = self.optimizers()\n        opt.zero_grad()\n        self.manual_backward(loss)\n        opt.step()\n\n        self.log(\"model_loss\", loss, prog_bar=True)\n        self.model_loss_recorder.update(loss.data)\n\n    def on_train_epoch_end(self):\n        self.model_loss.append(self.model_loss_recorder.show().data.cpu().numpy())\n        self.model_loss_recorder = AvgMeter()\n\n    def validation_step(self, batch, batch_nb):\n        x, y = batch\n        self.fid.update((x + 1.0) / 2.0, real=True)\n\n        _x = self.sample(x.shape[0], y)\n        self.fid.update((_x + 1.0) / 2.0, real=False)\n\n        fid = self.fid.compute().data.cpu()\n        self.log(\"val_fid\", fid, prog_bar=True)\n        self.val_fid_recorder.update(fid)\n\n    def on_validation_epoch_end(self):\n        self.val_fid.append(self.val_fid_recorder.show().data.cpu().numpy())\n        self.val_fid_recorder = AvgMeter()\n\n    def test_step(self, batch, batch_nb):\n        x, y = batch\n        self.fid.update((x + 1.0) / 2.0, real=True)\n\n        _x = self.sample(x.shape[0], y)\n        self.fid.update((_x + 1.0) / 2.0, real=False)\n\n    def on_test_epoch_end(self):\n        fid = self.fid.compute().data.cpu()\n        self.log(\"test_fid\", fid, prog_bar=False, logger=True)\n\n    def on_train_end(self):\n        # Plot training loss\n        loss_img_file = f\"/content/{MODEL_NAME}_loss_plot.png\"\n        plt.plot(self.model_loss, color=\"r\")\n        plt.title(\"Loss Curves\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.grid()\n        plt.savefig(loss_img_file)\n        plt.clf()\n        img = cv2.imread(loss_img_file)\n        cv2_imshow(img)\n\n        # Plot FID metric over epochs\n        evaluation_metric_img_file = f\"/content/{MODEL_NAME}_fid_plot.png\"\n        plt.plot(self.val_fid[1:], color=\"b\")\n        plt.title(\"FID Curves\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"FID\")\n        plt.grid()\n        plt.savefig(evaluation_metric_img_file)\n        plt.clf()\n        img = cv2.imread(evaluation_metric_img_file)\n        cv2_imshow(img)\n\n    def train_dataloader(self):\n        return data.DataLoader(\n            dataset=TrainDataset,\n            batch_size=self.batch_size,\n            shuffle=True,\n            drop_last=True\n        )\n\n    def val_dataloader(self):\n        return data.DataLoader(\n            dataset=ValDataset,\n            batch_size=2,\n            shuffle=True,\n            drop_last=True\n        )\n\n    def test_dataloader(self):\n        return data.DataLoader(\n            dataset=TestDataset,\n            batch_size=2,\n            shuffle=True,\n            drop_last=True\n        )\n\n    def configure_optimizers(self):\n        return [optim.AdamW(self.parameters(), lr=self.lr)]\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:09:06.989588Z","iopub.execute_input":"2025-05-26T22:09:06.989938Z","iopub.status.idle":"2025-05-26T22:09:07.014933Z","shell.execute_reply.started":"2025-05-26T22:09:06.989912Z","shell.execute_reply":"2025-05-26T22:09:07.013933Z"}},"outputs":[],"execution_count":33},{"id":"bd7b215a","cell_type":"code","source":"MODEL_NAME = DiffusionModel.__name__","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:09:10.494550Z","iopub.execute_input":"2025-05-26T22:09:10.495296Z","iopub.status.idle":"2025-05-26T22:09:10.499232Z","shell.execute_reply.started":"2025-05-26T22:09:10.495269Z","shell.execute_reply":"2025-05-26T22:09:10.498439Z"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1747758142802,"user":{"displayName":"Joshua Tettey Teye","userId":"03989818565665752308"},"user_tz":0},"id":"9cngPQKTtyGV","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":34},{"id":"6fddae28","cell_type":"code","source":"load_pretrained = False  # Set to True to load from a saved checkpoint\ncheckpoint_path = None\n\nif load_pretrained:\n    print(\"Loading pre-weights...\")\n\n    # Load the pretrained model from a checkpoint file using PyTorch Lightning\n    model = DiffusionModel.load_from_checkpoint(\n        checkpoint_path=checkpoint_path,  # Path to the saved checkpoint\n\n        # Map the model to the appropriate device (GPU if available, else CPU)\n        map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n\n        # Required arguments for rebuilding the model instance (must match constructor)\n        forward_model=FORWARD(),         # Forward diffusion scheduler\n        backward_model=BACKWARD(),       # Backward (UNet) model\n        batch_size=BATCH_SIZE,           # Batch size during training/inference\n        lr=LR,                           # Learning rate\n        max_epoch=MAX_EPOCH,             # Total training epochs\n    )\nelse:\n    # Fresh initialization of the model from scratch\n    model = DiffusionModel(FORWARD(), BACKWARD(), 32, LR, MAX_EPOCH)","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:09:11.802832Z","iopub.execute_input":"2025-05-26T22:09:11.803147Z","iopub.status.idle":"2025-05-26T22:09:12.429342Z","shell.execute_reply.started":"2025-05-26T22:09:11.803129Z","shell.execute_reply":"2025-05-26T22:09:12.428423Z"}},"outputs":[],"execution_count":35},{"id":"60b8532b","cell_type":"markdown","source":"## **Training**","metadata":{"id":"boRJQCOJdFXa","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"id":"0f2d1146","cell_type":"code","source":"# Define a checkpoint callback to save the best model based on validation FID\ncheckpoint = ModelCheckpoint(\n    monitor='val_fid',            # Metric to monitor\n    dirpath=CHECKPOINT_DIR,       # Directory to save checkpoint files\n    mode='min',                   # Lower FID = better\n)\n\n# Create a Lightning Trainer object with desired settings\ntrainer = Trainer(\n    accelerator=\"auto\",           # Automatically choose GPU/CPU\n    devices=1,                    # Use 1 GPU or 1 CPU\n    max_epochs=30,                # Total training epochs\n    callbacks=[checkpoint],       # Use ModelCheckpoint callback\n    log_every_n_steps=5,          # Log metrics every 5 steps\n    check_val_every_n_epoch=5,    # Run validation every 5 epochs\n)\n\n# Begin training the model\ntrainer.fit(model)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:09:20.367595Z","iopub.execute_input":"2025-05-26T22:09:20.368105Z"},"id":"96vkBWERjAwH","outputId":"0824364f-0cf2-4a95-c210-ab8d220e7daf","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"2025-05-26 22:09:24.826272: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748297365.246892      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748297365.374367      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nINFO: \n  | Name           | Type                     | Params | Mode \n--------------------------------------------------------------------\n0 | forward_model  | NoiseScheduler           | 0      | train\n1 | backward_model | UNet                     | 5.8 M  | train\n2 | fid            | FrechetInceptionDistance | 23.9 M | train\n--------------------------------------------------------------------\n5.8 M     Trainable params\n23.9 M    Non-trainable params\n29.7 M    Total params\n118.768   Total estimated model params size (MB)\n225       Modules in train mode\n298       Modules in eval mode\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"609d9e09a1274dd984d073a38c96f6e4"}},"metadata":{}}],"execution_count":null},{"id":"1c4aab6a","cell_type":"markdown","source":"## **Testing**","metadata":{"id":"rx7WK_KxdFhM","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"id":"f8b99c44","cell_type":"code","source":"# Rename the best model checkpoint to a custom file name for easier identification\nos.rename(\n    checkpoint.best_model_path,                                     # Original path (auto-named by PyTorch Lightning)\n    os.path.join(CHECKPOINT_DIR, f\"{MODEL_NAME}_best.ckpt\")         # New path with custom name\n)\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:06:31.479855Z","iopub.status.idle":"2025-05-26T22:06:31.480135Z","shell.execute_reply.started":"2025-05-26T22:06:31.480011Z","shell.execute_reply":"2025-05-26T22:06:31.480022Z"}},"outputs":[],"execution_count":null},{"id":"ea134210","cell_type":"code","source":"# Set model to evaluation mode to disable dropout, batchnorm updates, etc.\nmodel.eval()\n\n# Run the test step using the best saved checkpoint\ntrainer.test(\n    ckpt_path=os.path.join(CHECKPOINT_DIR, f\"{MODEL_NAME}_best.ckpt\")\n)\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:06:31.481181Z","iopub.status.idle":"2025-05-26T22:06:31.481524Z","shell.execute_reply.started":"2025-05-26T22:06:31.481388Z","shell.execute_reply":"2025-05-26T22:06:31.481403Z"}},"outputs":[],"execution_count":null},{"id":"bc68ee56","cell_type":"markdown","source":"## **Inference**","metadata":{"id":"JfvehFoAiW16","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"id":"5055ca71","cell_type":"markdown","source":"### **Utils**","metadata":{"id":"9U0lYLLUR23C","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"id":"f85eb07d","cell_type":"code","source":"CHECKPOINT_INFERENCE = os.path.join(CHECKPOINT_DIR, f\"{MODEL_NAME}_best.ckpt\")","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:06:31.482167Z","iopub.status.idle":"2025-05-26T22:06:31.482571Z","shell.execute_reply.started":"2025-05-26T22:06:31.482376Z","shell.execute_reply":"2025-05-26T22:06:31.482394Z"},"id":"ndWwtFfmJZ_F","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"3d3ab4af","cell_type":"code","source":"N_SAMPLE = 5","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:06:31.483383Z","iopub.status.idle":"2025-05-26T22:06:31.483762Z","shell.execute_reply.started":"2025-05-26T22:06:31.483563Z","shell.execute_reply":"2025-05-26T22:06:31.483582Z"},"id":"srJUwJm8ktjs","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"f31c0ff8","cell_type":"code","source":"# Load the pretrained diffusion model from a checkpoint\nmodel = DiffusionModel.load_from_checkpoint(\n    checkpoint_path=CHECKPOINT_INFERENCE,  # Path to your saved .ckpt file\n    map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\",  # Load onto the correct device\n    forward_model=FORWARD(),         # Reconstruct the forward noise scheduler\n    backward_model=BACKWARD(),       # Reconstruct the backward UNet\n    batch_size=BATCH_SIZE,           # Must match what was used during training\n    lr=LR,                           # Learning rate used at training\n    max_epoch=MAX_EPOCH              # Total epochs used in training\n)\n\n# Set model to evaluation mode to disable training-specific behavior (e.g., dropout)\nmodel.eval()\n\nprint(\"done\")","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T22:06:31.485953Z","iopub.status.idle":"2025-05-26T22:06:31.486250Z","shell.execute_reply.started":"2025-05-26T22:06:31.486119Z","shell.execute_reply":"2025-05-26T22:06:31.486132Z"}},"outputs":[],"execution_count":null},{"id":"b08bfe81","cell_type":"markdown","source":"### Visualize","metadata":{"id":"ym6UVoVoR6NE","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}},{"id":"f3069fc6","cell_type":"code","source":"samples_per_class = 10  # Number of samples to generate per class\n\nfor class_id in range(0, 2):  # Loop through all classes (e.g., 0 and 1)\n    # Create one-hot class label and move to CUDA\n    class_label = F.one_hot(torch.tensor(class_id, dtype=torch.int64), num_classes=N_CLASSES).float().to(\"cuda\")\n    class_label = class_label.unsqueeze(0)  # Shape: [1, N_CLASSES]\n    \n    print(f\"\\nClass {class_id} one-hot:\", class_label)\n\n    model.eval()  # Ensure model is in inference mode\n\n    for sample_idx in range(samples_per_class):\n        plt.figure(figsize=(5, 5))  # Set figure size for clarity\n\n        # Generate 1 sample with the given class condition\n        generated = model.sample(\n            n=1,\n            context=class_label,\n            progress=False,\n            verbose=False,\n            n_progress=5\n        )\n\n        # Extract image from batch (shape: [1, C, H, W]) ‚Üí [C, H, W]\n        image = generated[0] if isinstance(generated, list) else generated[0, :, :, :]\n\n        # Show the generated image\n        show_tensor_image(image)","metadata":{"execution":{"iopub.status.busy":"2025-05-26T22:06:31.487163Z","iopub.status.idle":"2025-05-26T22:06:31.487494Z","shell.execute_reply.started":"2025-05-26T22:06:31.487310Z","shell.execute_reply":"2025-05-26T22:06:31.487321Z"},"id":"e9STip-Oh5sY","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}