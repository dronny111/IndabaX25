{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28df5579",
   "metadata": {},
   "source": [
    "# **Leveraging LLMs for Text Generation and Summarization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c363790",
   "metadata": {},
   "source": [
    "# **Table of Contents**\n",
    "\n",
    "1.   [Transformer](#The Transformer Architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddceaa6",
   "metadata": {},
   "source": [
    "# PART I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5898b8a3",
   "metadata": {},
   "source": [
    "### Learning Objectives for Part 1:\n",
    "- Understand the architecture of modern LLMs and their categories\n",
    "- Implement extractive summarization techniques\n",
    "- Establish evaluation metrics for summarization quality\n",
    "- Begin building a multi-stage summarization pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9504111f",
   "metadata": {},
   "source": [
    "# Architectural Overview of LLMs\n",
    "\n",
    "## The Transformer Architecture\n",
    "\n",
    "The transformer architecture revolutionized NLP when introduced in the paper \"Attention is All You Need\" (Vaswanathan et al., 2017).\n",
    "\n",
    "Transformer Architecture\n",
    "<img src='images/transformer architecture.png' width=1000 height=700>\n",
    "\n",
    "<!-- <img src=\"https://drive.google.com/uc?export=view&id=1LKXGpOboC75U6y63z1ZAJeCyw06BkLYH\" width=\"1000\" height=\"700\"> -->\n",
    "\n",
    "\n",
    "\n",
    "### Key Components:\n",
    "- **Self-Attention Mechanism**: Allows the model to weigh the importance of different words in context\n",
    "- **Positional Encoding**: Helps the model understand word order\n",
    "- **Feed-Forward Networks**: Process the representations from attention layers\n",
    "- **Layer Normalization**: Stabilizes training\n",
    "\n",
    "### Why Transformers Work Well for Summarization:\n",
    "- Can handle long-range dependencies between words\n",
    "- Process entire sequences in parallel (unlike RNNs)\n",
    "- Capture contextual relationships essential for understanding document structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b85d1e",
   "metadata": {},
   "source": [
    "# Categories of LLMs\n",
    "\n",
    "LLMs come in different architectural variants, each with strengths for different tasks:\n",
    "\n",
    "## 1. Decoder-Only Models (Autoregressive)\n",
    "- Examples: GPT series, LLaMA, Claude\n",
    "- Trained to predict the next token\n",
    "- **Strengths for summarization**: Creative text generation, coherent narrative\n",
    "- **Weaknesses**: May hallucinate or add information not in source\n",
    "\n",
    "## 2. Encoder-Only Models\n",
    "- Examples: BERT, RoBERTa\n",
    "- Trained on masked language modeling\n",
    "- **Strengths for summarization**: Understanding document context, good for extractive summarization\n",
    "- **Weaknesses**: Not designed for generation\n",
    "\n",
    "## 3. Encoder-Decoder Models\n",
    "- Examples: T5, BART\n",
    "- Trained on sequence-to-sequence tasks\n",
    "- **Strengths for summarization**: Balanced understanding and generation, ideal for abstractive summarization\n",
    "- **Weaknesses**: Larger compute requirements\n",
    "\n",
    "### Which architecture is best for summarization?\n",
    "It depends on the task! We'll explore the tradeoffs throughout this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae68707",
   "metadata": {},
   "source": [
    "# Quick Exercise: Choosing the Right Architecture\n",
    "\n",
    "For each summarization scenario below, identify which model architecture (decoder-only, encoder-only, or encoder-decoder) would be most appropriate and why:\n",
    "\n",
    "1. Generating creative article summaries with novel phrasing\n",
    "2. Extracting key sentences from legal documents\n",
    "3. Translating and summarizing simultaneously\n",
    "4. Creating concise bullet points from meeting transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e11c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to view the answers\n",
    "print(\"1. Decoder-only models excel at creative generation but may add details.\")\n",
    "\n",
    "print(\"2. Encoder-only models like BERT are great at understanding document context.\")\n",
    "\n",
    "print(\"3. Encoder-decoder models like T5 are designed for tasks requiring both understanding and generation.\")\n",
    "\n",
    "print(\"4. This could use either encoder-only for extraction or encoder-decoder for concise reformulation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2bfed6",
   "metadata": {},
   "source": [
    "<img src=\"images/text summarization.jpg\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb6ec0c",
   "metadata": {},
   "source": [
    "# Extractive Summarization\n",
    "\n",
    "Extractive summarization selects the most important sentences from the orginal text to create the summary.\n",
    "\n",
    "## The Basic Process:\n",
    "1. Score sentences based on importance\n",
    "2. Select top-scoring sentences using ranking algorithm (TF IDF, SVM, etc)\n",
    "3. Arrange in coherent order (usually original order)\n",
    "\n",
    "## Advantages:\n",
    "- Factually accurate (uses original text)\n",
    "- Computationally efficient\n",
    "- Works well for objective content\n",
    "\n",
    "## Disadvantages:\n",
    "- May be disconnected or redundant\n",
    "- Cannot reformulate or simplify complex content\n",
    "- Limited by quality of source material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc021786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if is_colab():\n",
    "    from google.colab import userdata\n",
    "    api_key = userdata.get('OPENROUTER_API_KEY')\n",
    "else:\n",
    "  from dotenv import load_dotenv\n",
    "  load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dfba888d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import the required libraries\n",
    "import nltk\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords') \n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "\n",
    "def extractive_summary(text, ratio=0.3):\n",
    "    # Tokenize the text into individual sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Tokenize each sentence into individual words and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    words = [word.lower() for word in word_tokenize(text) if word.lower() not in stop_words and word.isalnum()] # This removes any stop words and non-alphanumeric characters from the resulting list of words and converts them all to lowercase\n",
    "\n",
    "    # Compute the frequency of each word\n",
    "    word_freq = Counter(words)\n",
    "\n",
    "    # Compute the score for each sentence based on the frequency of its words\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        sentence_words = [word.lower() for word in word_tokenize(sentence) if word.lower() not in stop_words and word.isalnum()]\n",
    "        sentence_score = sum([word_freq[word] for word in sentence_words])\n",
    "        if len(sentence_words) < 20: # to filter short sentences\n",
    "            sentence_scores[sentence] = sentence_score\n",
    "\n",
    "    # Compute the number of sentences to include in the summary\n",
    "    num_sentences = max(1, int(len(sentences) * ratio))\n",
    "\n",
    "\n",
    "    summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
    "    summary = ' '.join(summary_sentences)\n",
    "\n",
    "    return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "# Load a Ghanaian news article\n",
    "with open('article.txt', 'r', encoding='utf-8') as f:\n",
    "    ghana_article = f.read()\n",
    "    \n",
    "# Print article length\n",
    "print(f\"Article contains {len(sent_tokenize(ghana_article))} sentences and {len(ghana_article.split())} words\")\n",
    "summary= extractive_summary(ghana_article, ratio=0.3)\n",
    "print(f\"SUMMARY\\n{summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f13ebb",
   "metadata": {},
   "source": [
    "# Evaluation Metrics for Summarization\n",
    "\n",
    "How do we know if our summaries are good? Let's implement some common evaluation metrics:\n",
    "\n",
    "## ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "- Measures overlap between machine-generated summary and reference summary\n",
    "- ROUGE-N: N-gram recall\n",
    "- ROUGE-L: Longest Common Subsequence \n",
    "\n",
    "<img src='images/Rouge L.jpeg' width=700>\n",
    "\n",
    "LCS is the longest set of ordered tokens that occurs in both sequences (Ref, Gen)\n",
    "\n",
    "## BLEU (Bilingual Evaluation Understudy)\n",
    "- Originally designed for translation, but used for summarization\n",
    "- Precision-focused (how many generated n-grams appear in reference)\n",
    "\n",
    "## BERTScore\n",
    "- Uses contextual embeddings to compute similarity\n",
    "- Better semantic understanding than n-gram methods\n",
    "\n",
    "## Human Evaluation Dimensions\n",
    "- **Relevance**: Does it capture important information?\n",
    "- **Coherence**: Does it flow logically?\n",
    "- **Fluency**: Is it grammatically correct?\n",
    "- **Factuality**: Does it contain errors or hallucinations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eadeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing ROUGE for evaluation\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_rouge_n(reference, candidate, n=1):\n",
    "    \"\"\"Calculate ROUGE-N score (n-gram F1 score)\"\"\"\n",
    "    # Tokenize into words\n",
    "    ref_tokens = word_tokenize(reference.lower())\n",
    "    cand_tokens = word_tokenize(candidate.lower())\n",
    "    \n",
    "    # Generate n-grams\n",
    "    def get_ngrams(tokens, n):\n",
    "        ngrams = []\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngrams.append(tuple(tokens[i:i+n]))\n",
    "        return ngrams\n",
    "    \n",
    "    ref_ngrams = get_ngrams(ref_tokens, n)\n",
    "    cand_ngrams = get_ngrams(cand_tokens, n)\n",
    "    \n",
    "    # Count ngrams\n",
    "    ref_count = Counter(ref_ngrams)\n",
    "    cand_count = Counter(cand_ngrams)\n",
    "    \n",
    "    # Count matches\n",
    "    matches = 0\n",
    "    for ngram in cand_count:\n",
    "        matches += min(cand_count[ngram], ref_count[ngram])\n",
    "    \n",
    "    # Calculate recall\n",
    "    if len(ref_ngrams) == 0:\n",
    "        recall =  0\n",
    "    recall =  matches / len(ref_ngrams)\n",
    "\n",
    "    # calculate precision\n",
    "    if len(cand_ngrams) == 0:\n",
    "        precision = 0\n",
    "    precision = matches / len(cand_ngrams)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1*100\n",
    "\n",
    "\n",
    "# Let's create a simple function to evaluate our summaries\n",
    "def evaluate_summary(reference, candidate):\n",
    "    \"\"\"Evaluate a summary using multiple metrics\"\"\"\n",
    "    scores = {\n",
    "        'ROUGE-1': calculate_rouge_n(reference, candidate, 1),\n",
    "        'ROUGE-2': calculate_rouge_n(reference, candidate, 2),\n",
    "    }\n",
    "    \n",
    "    # Add readability metric: average words per sentence\n",
    "    cand_sentences = sent_tokenize(candidate)\n",
    "    avg_sentence_length = len(word_tokenize(candidate)) / max(1, len(cand_sentences))\n",
    "    scores['Avg Words/Sentence'] = avg_sentence_length\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c667d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try evaluating our summary against a reference summary\n",
    "\n",
    "with open('reference_summary.txt', 'r', encoding='utf-8') as f:\n",
    "    reference_summary = f.read()\n",
    "\n",
    "\n",
    "# Evaluate our extractive summary against the reference\n",
    "evaluation_scores = evaluate_summary(reference_summary, summary)\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "for metric, score in evaluation_scores.items():\n",
    "    print(f\"{metric}: {score:.4f}\")\n",
    "\n",
    "# Let's try different summary ratios and compare\n",
    "ratios = [0.2, 0.3, 0.4, 0.5]\n",
    "ratio_results = []\n",
    "\n",
    "for ratio in ratios:\n",
    "    test_summary = extractive_summary(ghana_article, ratio=ratio)\n",
    "    scores = evaluate_summary(reference_summary, test_summary)\n",
    "    scores['ratio'] = ratio\n",
    "    scores['length'] = len(test_summary.split())\n",
    "    ratio_results.append(scores)\n",
    "\n",
    "# Display comparison\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(ratio_results)\n",
    "print(\"\\nComparison of Different Summary Ratios:\")\n",
    "print(results_df[['ratio', 'length', 'ROUGE-1', 'ROUGE-2']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a04f07",
   "metadata": {},
   "source": [
    "# Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e87d459",
   "metadata": {},
   "source": [
    "# Abstractive Summarization & Control Parameters\n",
    "\n",
    "\n",
    "## What is Abstractive Summarization?\n",
    "\n",
    "Abstractive summarization involves:\n",
    "- Understanding the source content deeply\n",
    "- Identifying key concepts and relationships\n",
    "- Generating new text by paraphrasing existing text\n",
    "- Condensing information in ways that extraction cannot\n",
    "\n",
    "## Why Use Encoder-Decoder Models?\n",
    "\n",
    "While autoregressive (decoder-only) models like GPT can perform abstractive summarization, encoder-decoder models like T5 and BART offer specific advantages:\n",
    "\n",
    "1. **Bidirectional encoding**: The encoder comprehends the entire document before generation begins\n",
    "2. **Source-target separation**: Clear distinction between understanding and generation\n",
    "3. **Cross-attention mechanism**: Decoder can directly reference source material during generation\n",
    "4. **Training objectives**: Pre-trained specifically for tasks that include summarization\n",
    "5. **Control mechanisms**: Easier to implement length constraints and other controls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5b7ab5",
   "metadata": {},
   "source": [
    "# Key Models for Abstractive Summarization\n",
    "\n",
    "## BART (Bidirectional and Auto-Regressive Transformers)\n",
    "- Combines bidirectional encoder (like BERT) with autoregressive decoder\n",
    "- Pre-trained on denoising tasks, including text infilling and sentence shuffling\n",
    "- Particularly effective for summarization tasks\n",
    "\n",
    "## T5 (Text-to-Text Transfer Transformer)\n",
    "- Treats all NLP tasks as \"text-to-text\" problems\n",
    "- Consistent performance across summarization, translation, classification, etc.\n",
    "- Uses a \"prefix\" to specify the task (e.g., \"summarize:\")\n",
    "\n",
    "## Pegasus\n",
    "- Specifically pre-trained for abstractive summarization\n",
    "- Uses \"gap sentences\" pre-training, masking important sentences during training\n",
    "- Optimized for news summarization tasks\n",
    "\n",
    "## Comparing with Autoregressive Models (e.g., GPT)\n",
    "\n",
    "| Aspect | Encoder-Decoder | Autoregressive |\n",
    "|--------|-----------------|----------------|\n",
    "| Source understanding | Bidirectional | Primarily left-to-right |\n",
    "| Memory of source | Direct attention | Must retain in context |\n",
    "| Training objective | Often summarization-specific | General next-token prediction |\n",
    "| Length control | Easier to implement | Requires special techniques |\n",
    "| Hallucination risk | Lower (with cross-attention) | Higher |\n",
    "| Flexibility | Task-specific | More general-purpose |\n",
    "\n",
    "In practice, the lines are blurring as models evolve. Modern autoregressive models can achieve excellent summarization through few-shot prompting and other techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe4481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import time\n",
    "import pickle\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, BartForConditionalGeneration, BartTokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "class T5Summarizer:\n",
    "    \"\"\"T5-based abstractive summarizer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"t5-small\", device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.prefix = \"summarize: \"\n",
    "        self.checkpoint_name = f\"{model_name.replace('/', '_')}_t5\"\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load model and tokenizer with progress tracking\"\"\"\n",
    "        print(f\"Loading {self.model_name}...\")\n",
    "            \n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(self.model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(self.model_name).to(self.device)\n",
    "        \n",
    "        \n",
    "    def summarize(self, text, max_length=150, min_length=50, length_penalty=2.0, \n",
    "                  num_beams=4, early_stopping=True):\n",
    "        \"\"\"Generate summary with T5, adding the task prefix\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            self.load_model()\n",
    "            \n",
    "        # Add prefix for T5\n",
    "        prefixed_text = self.prefix + text\n",
    "        \n",
    "        # Tokenize input text\n",
    "        inputs = self.tokenizer(prefixed_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(self.device)\n",
    "        \n",
    "        # Generate summary\n",
    "        summary_ids = self.model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            length_penalty=length_penalty,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=early_stopping\n",
    "        )\n",
    "        \n",
    "        # Decode and return summary\n",
    "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        return summary\n",
    "\n",
    "\n",
    "class BartSummarizer:\n",
    "    \"\"\"BART-based abstractive summarizer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"facebook/bart-base\", device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.checkpoint_name = f\"{model_name.replace('/', '_')}_bart\"\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load model and tokenizer with progress tracking\"\"\"\n",
    "        print(f\"Loading {self.model_name}...\")\n",
    "            \n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = BartForConditionalGeneration.from_pretrained(self.model_name).to(self.device)\n",
    "        \n",
    "    def summarize(self, text, max_length=150, min_length=50, length_penalty=2.0, \n",
    "                  num_beams=4, early_stopping=True):\n",
    "        \"\"\"Generate an abstractive summary with BART\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            self.load_model()\n",
    "            \n",
    "        # Tokenize input text\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True).to(self.device)\n",
    "        \n",
    "        # Generate summary\n",
    "        summary_ids = self.model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            length_penalty=length_penalty,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=early_stopping\n",
    "        )\n",
    "        \n",
    "        # Decode and return summary\n",
    "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "class AutoregressiveSumarizer:\n",
    "\n",
    "    def __init__(self, model_name=\"google/gemini-2.0-flash-lite-001\"):\n",
    "        self.model = model_name\n",
    "        self.api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "        self.api_url = os.getenv(\"OPENROUTER_API_URL\")\n",
    "        self.client = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=self.api_key)\n",
    "\n",
    "    def summarize(self, text, system_prompt, temperature=0.8, max_tokens=500):\n",
    "        \"\"\"Generate summary using autoregressive model\"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            model=self.model,\n",
    "            extra_body={},\n",
    "        )\n",
    "        \n",
    "        # Extract and return the summary\n",
    "        summary = response.choices[0].message.content\n",
    "        return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e95cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try out our abstractive summarizers\n",
    "\n",
    "# Let's create our summarizers\n",
    "t5_summarizer = T5Summarizer(\"t5-small\")  # \n",
    "bart_summarizer = BartSummarizer(\"facebook/bart-base\")\n",
    "autoregressive_summarizer = AutoregressiveSumarizer()\n",
    "\n",
    "# Generate summaries\n",
    "t5_summary = t5_summarizer.summarize(\n",
    "    ghana_article,\n",
    "    max_length=100,  # Target length in tokens\n",
    "    min_length=30,\n",
    "    num_beams=4      # Beam search for better quality\n",
    ")\n",
    "\n",
    "bart_summary = bart_summarizer.summarize(\n",
    "    ghana_article,\n",
    "    max_length=100,\n",
    "    min_length=30,\n",
    "    num_beams=4\n",
    ")\n",
    "\n",
    "autoregressive_summary = autoregressive_summarizer.summarize(\n",
    "    ghana_article,\n",
    "    \"Summarize the following text in a concise manner.\",\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# Let's also get our extractive summary for comparison\n",
    "extractive_summary= extractive_summary(ghana_article, ratio=0.3)\n",
    "\n",
    "# Display all summaries\n",
    "print(\"Original Text Length:\", len(ghana_article.split()))\n",
    "print(\"\\n--- T5 Summary ---\")\n",
    "print(t5_summary)\n",
    "print(\"\\nLength:\", len(t5_summary.split()))\n",
    "\n",
    "print(\"\\n--- BART Summary ---\")\n",
    "print(bart_summary)\n",
    "print(\"\\nLength:\", len(bart_summary.split()))\n",
    "\n",
    "print(\"\\n--- AutoRegressive Summary ---\")\n",
    "print(autoregressive_summary)\n",
    "print(\"\\nLength:\", len(autoregressive_summary.split()))\n",
    "\n",
    "print(\"\\n--- Extractive Summary ---\")\n",
    "print(extractive_summary)\n",
    "print(\"\\nLength:\", len(extractive_summary.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b104b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same news file so we use the same reference summary\n",
    "# Evaluate all summaries\n",
    "t5_scores = evaluate_summary(reference_summary, t5_summary)\n",
    "bart_scores = evaluate_summary(reference_summary, bart_summary)\n",
    "autoregressive_scores = evaluate_summary(reference_summary, autoregressive_summary)\n",
    "extractive_scores = evaluate_summary(reference_summary, extractive_summary)\n",
    "\n",
    "# Compare scores\n",
    "summary_comparison = pd.DataFrame({\n",
    "    'T5': t5_scores,\n",
    "    'BART': bart_scores,\n",
    "    'AutoRegressive': autoregressive_scores,\n",
    "    'Extractive': extractive_scores\n",
    "})\n",
    "\n",
    "print(\"Objective Metrics Comparison:\")\n",
    "print(summary_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1c271d",
   "metadata": {},
   "source": [
    "# Controllable Summarization\n",
    "\n",
    "One of the major advantages of modern summarization systems is the ability to control various aspects of the generated summaries:\n",
    "\n",
    "## Common Control Parameters:\n",
    "\n",
    "1. **Length**: Controlling how long or short the summary should be\n",
    "2. **Style**: Formal vs. casual, simple vs. technical\n",
    "3. **Focus**: Emphasizing particular topics or aspects\n",
    "4. **Structure**: Bullet points, narrative, or question-answering\n",
    "\n",
    "## How to Implement Controls:\n",
    "\n",
    "1. **Model-specific parameters**: Using built-in generation controls\n",
    "2. **Prompt engineering**: Adding instructional prefixes\n",
    "3. **Output filtering**: Post-processing generated summaries\n",
    "4. **Fine-tuning**: Training the model with examples of desired style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb1780e",
   "metadata": {},
   "source": [
    "Length control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50907a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing length control for our summarizers\n",
    "\n",
    "def generate_controlled_summaries(text, model, lengths=[50, 100, 200]):\n",
    "    \"\"\"Generate summaries of different controlled lengths\"\"\"\n",
    "    summaries = {}\n",
    "    \n",
    "    for length in lengths:\n",
    "        if isinstance(model, T5Summarizer):\n",
    "            summary = model.summarize(\n",
    "                text,\n",
    "                max_length=length,\n",
    "                min_length=max(10, int(length * 0.7)),  # At least 70% of max length\n",
    "                num_beams=4\n",
    "            )\n",
    "            summaries[f\"T5 (length={length})\"] = summary\n",
    "            \n",
    "        elif isinstance(model, BartSummarizer):\n",
    "            summary = model.summarize(\n",
    "                text,\n",
    "                max_length=length,\n",
    "                min_length=max(10, int(length * 0.7)),\n",
    "                num_beams=4\n",
    "            )\n",
    "            summaries[f\"BART (length={length})\"] = summary\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "# Let's generate summaries of different lengths\n",
    "length_controlled_t5 = generate_controlled_summaries(ghana_article, t5_summarizer, [50, 100, 150])\n",
    "length_controlled_bart = generate_controlled_summaries(ghana_article, bart_summarizer, [50, 100, 150])\n",
    "\n",
    "# Display and analyze length-controlled summaries\n",
    "print(\"Length-Controlled Summaries:\\n\")\n",
    "\n",
    "for name, summary in {**length_controlled_t5, **length_controlled_bart}.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(summary)\n",
    "    print(f\"Actual length: {len(summary.split())} words\")\n",
    "\n",
    "# Plotting the relationship between target length and actual length\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract target lengths and actual lengths\n",
    "target_lengths = []\n",
    "actual_lengths_t5 = []\n",
    "actual_lengths_bart = []\n",
    "\n",
    "for length in [50, 100, 150]:\n",
    "    target_lengths.append(length)\n",
    "    actual_lengths_t5.append(len(length_controlled_t5[f\"T5 (length={length})\"].split()))\n",
    "    actual_lengths_bart.append(len(length_controlled_bart[f\"BART (length={length})\"].split()))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(target_lengths, actual_lengths_t5, 'o-', label='T5')\n",
    "plt.plot(target_lengths, actual_lengths_bart, 'o-', label='BART')\n",
    "plt.plot(target_lengths, target_lengths, '--', label='Target=Actual', color='gray')\n",
    "plt.xlabel('Target Length (tokens)')\n",
    "plt.ylabel('Actual Length (words)')\n",
    "plt.title('Length Control Effectiveness')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.savefig('length_control.png')\n",
    "plt.close()\n",
    "\n",
    "from IPython.display import Image\n",
    "Image('length_control.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c556545a",
   "metadata": {},
   "source": [
    "Prompt Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5036e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing focus control through prompt engineering\n",
    "\n",
    "def focus_controlled_summary(text, model, focus_area):\n",
    "    \"\"\"Generate a summary focused on a specific aspect\"\"\"\n",
    "    \n",
    "    # Create a focused prompt\n",
    "    if isinstance(model, T5Summarizer):\n",
    "        focused_prompt = f\"summarize focusing on {focus_area}: {text}\"\n",
    "        summary = model.summarize(\n",
    "            focused_prompt,\n",
    "            max_length=100,\n",
    "            min_length=30,\n",
    "            num_beams=4\n",
    "        )\n",
    "    \n",
    "    elif isinstance(model, BartSummarizer):\n",
    "        # For BART, we need to be more creative since it doesn't have a prefix\n",
    "        # Append the instruction to the beginning of the text\n",
    "        focused_prompt = f\"Focus on {focus_area} in your summary. {text}\"\n",
    "        summary = model.summarize(\n",
    "            focused_prompt,\n",
    "            max_length=100,\n",
    "            min_length=30,\n",
    "            num_beams=4\n",
    "        )\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate summaries focused on different aspects\n",
    "focus_areas = [\"cost of living comparison\", \"Brain Drain vs. Nation Building\", \"unity and collective action\"]\n",
    "focused_summaries = {}\n",
    "\n",
    "for focus in focus_areas:\n",
    "    focused_summaries[f\"T5 (focus: {focus})\"] = focus_controlled_summary(ghana_article, t5_summarizer, focus)\n",
    "    focused_summaries[f\"BART (focus: {focus})\"] = focus_controlled_summary(ghana_article, bart_summarizer, focus)\n",
    "\n",
    "# Display focused summaries\n",
    "print(\"Focus-Controlled Summaries:\\n\")\n",
    "\n",
    "for name, summary in focused_summaries.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(summary)\n",
    "\n",
    "# Let's analyze how well the focus control worked\n",
    "import re\n",
    "\n",
    "def count_focus_related_words(text, focus):\n",
    "    \"\"\"Count words related to the focus area\"\"\"\n",
    "    # Create a simple keyword list for each focus area\n",
    "    focus_keywords = {\n",
    "        \"cost of living comparison\": [\"youth emigration\", \"greener pastures\", \"high unemployment\", \"limited opportunities\", \"collective responsibility\", \"development\"],\n",
    "        \"Brain Drain vs. Nation Building\": [\"affordable housing\", \"lower living expenses\", \"imported goods\",  \"entrepreneurship opportunities\"],\n",
    "        \"unity and collective action\": [ \"unity\", \"peace\", \"collaboration\",  \"technology leverage\", \"shared vision\", \"mutual respect\",\"national development\"]\n",
    "    }\n",
    "    \n",
    "    # Count occurrences of focus keywords\n",
    "    keywords = focus_keywords.get(focus, [])\n",
    "    count = sum(1 for keyword in keywords if re.search(r'\\b' + keyword + r'\\b', text.lower()))\n",
    "    \n",
    "    return count, len(keywords)\n",
    "\n",
    "# Analyze focus effectiveness\n",
    "focus_effectiveness = {}\n",
    "\n",
    "for focus in focus_areas:\n",
    "    t5_count, total = count_focus_related_words(\n",
    "        focused_summaries[f\"T5 (focus: {focus})\"], \n",
    "        focus\n",
    "    )\n",
    "    bart_count, _ = count_focus_related_words(\n",
    "        focused_summaries[f\"BART (focus: {focus})\"], \n",
    "        focus\n",
    "    )\n",
    "    \n",
    "    # Calculate percentage of focus keywords present\n",
    "    focus_effectiveness[focus] = {\n",
    "        \"T5\": f\"{t5_count}/{total} keywords ({t5_count/total*100:.1f}%)\",\n",
    "        \"BART\": f\"{bart_count}/{total} keywords ({bart_count/total*100:.1f}%)\"\n",
    "    }\n",
    "\n",
    "# Display focus effectiveness\n",
    "print(\"\\nFocus Control Effectiveness:\\n\")\n",
    "for focus, models in focus_effectiveness.items():\n",
    "    print(f\"Focus area: {focus}\")\n",
    "    print(f\"  T5: {models['T5']}\")\n",
    "    print(f\"  BART: {models['BART']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96331874",
   "metadata": {},
   "source": [
    "Style Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc92288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing style and structure control\n",
    "\n",
    "def style_controlled_summary(text, model, style):\n",
    "    \"\"\"Generate a summary with a specific style\"\"\"\n",
    "    \n",
    "    style_prompts = {\n",
    "        \"formal\": \"Generate a formal and technical summary of the following text:\",\n",
    "        \"simple\": \"Generate a simple summary using basic vocabulary and short sentences:\",\n",
    "        \"bullet_points\": \"Generate a summary in bullet point format highlighting key points:\",\n",
    "        \"question_answering\": \"Generate a summary in question and answer format about:\",\n",
    "        \"news_headline\": \"Write a news headline style summary of:\"\n",
    "    }\n",
    "    \n",
    "    prompt = f\"{style_prompts[style]} {text}\"\n",
    "    \n",
    "    if isinstance(model, T5Summarizer):\n",
    "        # For T5, replace the standard \"summarize:\" prefix\n",
    "        summary = model.summarize(\n",
    "            prompt,\n",
    "            max_length=120,\n",
    "            min_length=30,\n",
    "            num_beams=4\n",
    "        )\n",
    "    \n",
    "    elif isinstance(model, BartSummarizer):\n",
    "        summary = model.summarize(\n",
    "            prompt,\n",
    "            max_length=120,\n",
    "            min_length=30,\n",
    "            num_beams=4\n",
    "        )\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate summaries with different styles\n",
    "styles = [\"formal\", \"simple\", \"bullet_points\", \"question_answering\", \"news_headline\"]\n",
    "styled_summaries = {}\n",
    "\n",
    "for style in styles:\n",
    "    # Let's just use T5 for this demonstration\n",
    "    styled_summaries[f\"T5 (style: {style})\"] = style_controlled_summary(ghana_article, t5_summarizer, style)\n",
    "\n",
    "# Display styled summaries\n",
    "print(\"Style-Controlled Summaries:\\n\")\n",
    "\n",
    "for name, summary in styled_summaries.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(summary)\n",
    "\n",
    "# Simple readability assessment\n",
    "def assess_readability(text):\n",
    "    \"\"\"Calculate a simple readability score (average words per sentence)\"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return 0\n",
    "    \n",
    "    words = text.split()\n",
    "    avg_words_per_sentence = len(words) / len(sentences)\n",
    "    \n",
    "    return avg_words_per_sentence\n",
    "\n",
    "# Analyze style effectiveness\n",
    "style_assessment = {}\n",
    "\n",
    "for style, summary_key in zip(styles, styled_summaries.keys()):\n",
    "    summary = styled_summaries[summary_key]\n",
    "    \n",
    "    # Check if bullet points are present\n",
    "    has_bullets = \"â€¢\" in summary or \"-\" in summary.split() or any(line.strip().startswith(\"-\") for line in summary.split(\"\\n\"))\n",
    "    \n",
    "    # Check if questions are present\n",
    "    has_questions = \"?\" in summary\n",
    "    \n",
    "    # Assess readability\n",
    "    readability = assess_readability(summary)\n",
    "    \n",
    "    style_assessment[style] = {\n",
    "        \"Readability (words/sentence)\": f\"{readability:.1f}\",\n",
    "        \"Has bullet points\": \"Yes\" if has_bullets else \"No\",\n",
    "        \"Has questions\": \"Yes\" if has_questions else \"No\"\n",
    "    }\n",
    "\n",
    "# Display style assessment\n",
    "print(\"\\nStyle Control Assessment:\\n\")\n",
    "style_df = pd.DataFrame(style_assessment).T\n",
    "print(style_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab7b8da",
   "metadata": {},
   "source": [
    "# Building a Multi-Stage Summarization Pipeline\n",
    "\n",
    "Now let's combine the best of extractive and abstractive approaches to create a more effective summarization pipeline:\n",
    "\n",
    "## Benefits of a Multi-Stage Approach:\n",
    "\n",
    "1. **Handling longer documents**: Extractive methods can select relevant content from long texts\n",
    "2. **Improving factual accuracy**: Extractive first step preserves key facts\n",
    "3. **Computational efficiency**: Processing only relevant portions with resource-intensive models\n",
    "4. **Enhanced control**: Apply different strategies at different stages\n",
    "\n",
    "## Our Pipeline Design:\n",
    "\n",
    "1. **Stage 1**: Extractive selection of most relevant sentences\n",
    "2. **Stage 2**: Abstractive summarization of the extracted content\n",
    "3. **Stage 3**: Post-processing for quality control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc87208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a multi-stage summarization pipeline\n",
    "\n",
    "class MultiStageSummarizer:\n",
    "    \"\"\"Multi-stage pipeline combining extractive and abstractive summarization\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 extractive_summarizer,\n",
    "                 abstractive_summarizer,\n",
    "                 extractive_ratio=0.5):\n",
    "        \"\"\"Initialize the pipeline with component summarizers\"\"\"\n",
    "        self.extractive_summarizer = extractive_summarizer\n",
    "        self.abstractive_summarizer = abstractive_summarizer\n",
    "        self.extractive_ratio = extractive_ratio\n",
    "        \n",
    "    def summarize(self, text, max_length=100, min_length=30):\n",
    "        \"\"\"Generate a summary using the multi-stage pipeline\"\"\"\n",
    "        # Stage 1: Extractive summarization\n",
    "        print(\"Stage 1: Extractive summarization...\")\n",
    "        extractive_summary = self.extractive_summarizer(\n",
    "            text, \n",
    "            ratio=self.extractive_ratio\n",
    "        )\n",
    "        \n",
    "        # Check the intermediate result\n",
    "        print(f\"  Extracted ({len(extractive_summary.split())} words)\")\n",
    "        \n",
    "        # Stage 2: Abstractive summarization\n",
    "        print(\"Stage 2: Abstractive summarization...\")\n",
    "        if isinstance(self.abstractive_summarizer, T5Summarizer):\n",
    "            abstractive_summary = self.abstractive_summarizer.summarize(\n",
    "                extractive_summary,\n",
    "                max_length=max_length,\n",
    "                min_length=min_length,\n",
    "                num_beams=4\n",
    "            )\n",
    "        else:\n",
    "            abstractive_summary = self.abstractive_summarizer.summarize(\n",
    "                extractive_summary,\n",
    "                max_length=max_length,\n",
    "                min_length=min_length,\n",
    "                num_beams=4\n",
    "            )\n",
    "            \n",
    "        # Stage 3: Post-processing\n",
    "        print(\"Stage 3: Post-processing...\")\n",
    "        final_summary = self.post_process(abstractive_summary)\n",
    "        \n",
    "        return {\n",
    "            \"extractive_summary\": extractive_summary,\n",
    "            \"abstractive_summary\": abstractive_summary,\n",
    "            \"final_summary\": final_summary\n",
    "        }\n",
    "    \n",
    "    def post_process(self, summary):\n",
    "        \"\"\"Apply post-processing to improve summary quality\"\"\"\n",
    "        # Remove repeated phrases or sentences (a common issue)\n",
    "        sentences = sent_tokenize(summary)\n",
    "        unique_sentences = []\n",
    "        \n",
    "        for s in sentences:\n",
    "            # Skip nearly identical sentences (simple approach)\n",
    "            if not any(self.sentence_similarity(s, us) > 0.7 for us in unique_sentences):\n",
    "                unique_sentences.append(s)\n",
    "        \n",
    "        # Rejoin the unique sentences\n",
    "        processed_summary = ' '.join(unique_sentences)\n",
    "        \n",
    "        return processed_summary\n",
    "    \n",
    "    def sentence_similarity(self, s1, s2):\n",
    "        \"\"\"Calculate simple word overlap similarity between sentences\"\"\"\n",
    "        words1 = set(s1.lower().split())\n",
    "        words2 = set(s2.lower().split())\n",
    "        \n",
    "        if not words1 or not words2:\n",
    "            return 0\n",
    "            \n",
    "        overlap = words1.intersection(words2)\n",
    "        return len(overlap) / max(len(words1), len(words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28825ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "multistage_summarizer=MultiStageSummarizer(\n",
    "    extractive_summarizer=extractive_summary,\n",
    "    abstractive_summarizer=t5_summarizer,\n",
    "    extractive_ratio=0.3\n",
    ")\n",
    "result = multistage_summarizer.summarize(ghana_article)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe44d4b8",
   "metadata": {},
   "source": [
    "# Advanced Evaluation Beyond ROUGE\n",
    "\n",
    "While ROUGE and BLEU are standard metrics, they have limitations. Let's implement some more sophisticated evaluation techniques:\n",
    "\n",
    "## Factual Consistency\n",
    "Ensuring the summary doesn't contradict the source document\n",
    "\n",
    "## Semantic Similarity\n",
    "Using embeddings to measure semantic preservation\n",
    "\n",
    "## Readability Metrics\n",
    "Assessing how easy the summary is to understand\n",
    "\n",
    "These metrics can provide a more holistic view of summary quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bc49ef",
   "metadata": {},
   "source": [
    "# Part III"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69f26c",
   "metadata": {},
   "source": [
    "# Multimodal Summarization\n",
    "\n",
    "\n",
    "Multimodal summarization involves generating concise text that captures information from:\n",
    "- Text documents\n",
    "- Images\n",
    "- Tables and charts\n",
    "- Audio recordings\n",
    "- Video content\n",
    "\n",
    "## Approaches to Multimodal Summarization:\n",
    "\n",
    "1. **Pipeline Approach**: Process each modality separately, then combine\n",
    "2. **Unified Models**: Use multimodal models (like CLIP or GPT-4) that understand multiple modalities\n",
    "3. **Extraction + Description**: Extract elements from non-text modalities and describe them in text\n",
    "\n",
    "## Challenges:\n",
    "\n",
    "- Aligning information across modalities\n",
    "- Handling inconsistencies between modalities \n",
    "- Determining relative importance of different modalities\n",
    "- Technical complexity of processing multiple formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e721c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a multimodal summarizer for text + image data \n",
    "\n",
    "import requests\n",
    "import base64\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MultimodalSummarizer:\n",
    "    \"\"\"Multimodal summarizer using Llama-4 Vision capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key=None):\n",
    "        \"\"\"Initialize the multimodal summarizer with an OPENROUTER API key\"\"\"\n",
    "        # Get API key from environment variable if not provided\n",
    "        self.api_key = api_key or os.environ.get(\"OPENROUTER_API_KEY\", \"\")\n",
    "        if not self.api_key:\n",
    "            print(\"Warning: No OPENROUTER API key provided. Please set your OPENROUTER_API_KEY.\")\n",
    "        \n",
    "        self.api_url = os.getenv(\"OPENROUTER_API_URL\")\n",
    "        self.model = \"meta-llama/llama-4-maverick:free\"\n",
    "        \n",
    "    def encode_image(self, image_path):\n",
    "        \"\"\"Encode an image to base64 for API submission\"\"\"\n",
    "        # Check if it's a URL or local path\n",
    "        if image_path.startswith(('http://', 'https://')):\n",
    "            response = requests.get(image_path)\n",
    "            image = Image.open(BytesIO(response.content))\n",
    "            buffered = BytesIO()\n",
    "            image.save(buffered, format=\"JPEG\")\n",
    "            return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "        else:\n",
    "            with open(image_path, \"rb\") as image_file:\n",
    "                return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    def create_gpt4_payload(self, text, image_paths, max_tokens=500):\n",
    "        \"\"\"Create the API payload with text and images\"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that creates concise summaries from text and images.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": f\"Please create a comprehensive summary that combines information from the following text and images. Focus on integrating visual information with the text content.\\n\\nTEXT: {text}\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Add images to the content\n",
    "        for img_path in image_paths:\n",
    "            try:\n",
    "                base64_image = self.encode_image(img_path)\n",
    "                messages[1][\"content\"].append(\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                            \"detail\": \"high\"\n",
    "                        }\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {img_path}: {e}\")\n",
    "        \n",
    "        return {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "    \n",
    "    def summarize_multimodal(self, text, image_paths, max_tokens=500):\n",
    "        \"\"\"Generate a summary from text and images using GPT-4\"\"\"\n",
    "        if not self.api_key:\n",
    "            return {\"error\": \"No API key provided. Please set your OPENROUTER_API_KEY.\"}\n",
    "        \n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\"\n",
    "        }\n",
    "        \n",
    "        payload = self.create_gpt4_payload(text, image_paths, max_tokens)\n",
    "        \n",
    "        try:\n",
    "            # Simulate API call for workshop purposes\n",
    "            print(\"Making API call to model...\")\n",
    "\n",
    "            response = requests.post(self.api_url, headers=headers, data=payload)\n",
    "            result = response.json()\n",
    "            \n",
    "            return {\n",
    "                'text_source': text,\n",
    "                'image_paths': image_paths,\n",
    "                'combined_summary': result\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    def display_images(self, image_paths):\n",
    "        \"\"\"Display the images used in the multimodal summary\"\"\"\n",
    "        num_images = len(image_paths)\n",
    "        \n",
    "        if num_images == 0:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, num_images, figsize=(5*num_images, 5))\n",
    "        \n",
    "        if num_images == 1:\n",
    "            axes = [axes]  # Make it iterable for single image case\n",
    "            \n",
    "        for i, img_path in enumerate(image_paths):\n",
    "            try:\n",
    "                # Handle both URLs and local paths\n",
    "                if img_path.startswith(('http://', 'https://')):\n",
    "                    response = requests.get(img_path)\n",
    "                    img = Image.open(BytesIO(response.content))\n",
    "                else:\n",
    "                    img = Image.open(img_path)\n",
    "                \n",
    "                axes[i].imshow(img)\n",
    "                axes[i].set_title(f\"Image {i+1}\")\n",
    "                axes[i].axis('off')\n",
    "            except Exception as e:\n",
    "                axes[i].text(0.5, 0.5, f\"Error loading image: {e}\", \n",
    "                             ha='center', va='center', transform=axes[i].transAxes)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('multimodal_input.png')\n",
    "        plt.close()\n",
    "        \n",
    "        from IPython.display import Image\n",
    "        return Image('multimodal_input.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3103a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [\n",
    "    \"images/ghana_rural_solar.jpg\",\n",
    "    \"images/nzema_solar_plant.jpg\",\n",
    "    \"images/ghana_renewable_chart.png\"\n",
    "]\n",
    "\n",
    "multimodal_summarizer = MultimodalSummarizer()\n",
    "\n",
    "# Generate a multimodal summary\n",
    "multimodal_result = multimodal_summarizer.summarize_multimodal(\n",
    "    ghana_solar_text,\n",
    "    image_paths,\n",
    "    max_tokens=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b6a0ad",
   "metadata": {},
   "source": [
    "# Practical Exercise: Building Your Custom Summarization System\n",
    "\n",
    "Now it's your turn to build a complete summarization system by combining techniques we've explored.\n",
    "\n",
    "## Exercise Goals:\n",
    "1. Create a pipeline that combines multiple approaches\n",
    "2. Customize control parameters for your specific needs\n",
    "3. Evaluate results using advanced metrics\n",
    "4. Compare performance across different text types\n",
    "\n",
    "## Project Ideas:\n",
    "1. **News Summarizer Bot**: Create a system that retrieves and summarizes news articles on specific topics\n",
    "2. **Meeting Minutes Generator**: Transcribe and summarize meeting audio recordings\n",
    "3. **Research Paper Summarizer**: Generate summaries of academic papers with focus on methodology and results\n",
    "4. **Medical Conversation Summarizer**: Summarize doctor-patient conversations, creating dual summaries (technical for doctors, simplified for patients)\n",
    "5. **EHR Summarizer**: Create a system that generates longitudinal patient summaries from fragmented electronic health records, retrieving and synthesizing information across multiple visits, lab results, and clinical notes\n",
    "\n",
    "## Useful Resources:\n",
    "\n",
    "### Code and Libraries:\n",
    "- [ðŸ¤— Transformers Documentation](https://huggingface.co/docs/transformers/index)\n",
    "- [BART Model Card](https://huggingface.co/facebook/bart-large-cnn)\n",
    "- [T5 Model Card](https://huggingface.co/t5-base)\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
    "\n",
    "### Papers:\n",
    "- \"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\"\n",
    "- \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\"\n",
    "- \"Neural Abstractive Text Summarization with Sequence-to-Sequence Models\"\n",
    "\n",
    "### Datasets:\n",
    "- [CNN/Daily Mail Dataset](https://huggingface.co/datasets/cnn_dailymail)\n",
    "- [XSum Dataset](https://huggingface.co/datasets/xsum)\n",
    "- [Multi-News](https://huggingface.co/datasets/multi_news)\n",
    "\n",
    "### Evaluation Tools:\n",
    "- [ROUGE Implementation in Python](https://github.com/google-research/google-research/tree/master/rouge)\n",
    "- [BERTScore](https://github.com/Tiiiger/bert_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f76b4",
   "metadata": {},
   "source": [
    "# **Facilitator(s) Details**\n",
    "\n",
    "**Facilitator(s):**\n",
    "\n",
    "*   Name: Nana Sam Yeboah                       \n",
    "*   Email: nanayeb34@gmail.com\n",
    "*   LinkedIn: [Nana Sam Yeboah](https://www.linkedin.com/in/nana-sam-yeboah-0b664484)\n",
    "\n",
    "# \n",
    "\n",
    "*   Name: Audrey Eyram Agbeve\n",
    "*   Email: audreyagbeve02@gmail.com\n",
    "*   LinkedIn: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
