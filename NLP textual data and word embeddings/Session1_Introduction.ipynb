{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb271d8b",
   "metadata": {},
   "source": [
    "# Session 1: Introduction to Word Embeddings & Contextual Representations\n",
    "\n",
    "## ğŸ”¹ Welcome & Icebreaker\n",
    "\n",
    "Welcome to the NLP tutorial series! Over the next few sessions, weâ€™ll explore how machines understand word meaning using vector-based representations called **word embeddings**.\n",
    "\n",
    "### ğŸ‘‹ Objectives:\n",
    "- Introduce the idea of semantic similarity between words\n",
    "- Explore static vs. contextual word representations\n",
    "- Get hands-on with Word2Vec, GloVe, and BERT\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Icebreaker Activity: Word Association\n",
    "\n",
    "Letâ€™s start with a quick activity:\n",
    "\n",
    "1. When I say the word: **\"apple\"** say the **first word** that comes to your mind.\n",
    "2. Now, letâ€™s discuss:\n",
    "   - Why did different people associate different things?\n",
    "   - How does context shape our interpretation?\n",
    "\n",
    "### ğŸ’¬ Expected Responses:\n",
    "- â€œfruitâ€, â€œredâ€, â€œiPhoneâ€, â€œMacBookâ€, â€œSteve Jobsâ€, â€œtreeâ€â€¦\n",
    "\n",
    "This shows us that **words have multiple associations**, and their meanings change depending on context â€” exactly what word embeddings are designed to capture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca9e1c",
   "metadata": {},
   "source": [
    "## ğŸ¯ Introduction & Objectives\n",
    "\n",
    "In this session, we begin our journey into understanding how machines represent the meaning of words.\n",
    "\n",
    "**Why it matters:**\n",
    "- Natural Language Processing (NLP) applications like chatbots, translators, and search engines need to understand what words mean.\n",
    "- Computers can't 'read' in the human sense. They need numbers to process information. Word embeddings convert words into numbers â€” but smartly, by capturing relationships and meaning.\n",
    "\n",
    "**What we'll cover in this session:**\n",
    "1. What are Word Embeddings?\n",
    "2. Key concepts: Word2Vec and GloVe\n",
    "3. What are static embeddings, and how do they work?\n",
    "4. Why context matters: a sneak peek at contextual embeddings like BERT\n",
    "\n",
    "**Learning Outcome:**\n",
    "By the end of this session, you should be able to explain the concept of word embeddings, describe how static models work, and understand their limitations in capturing context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b183ac3",
   "metadata": {},
   "source": [
    "## ğŸ“š Basics of Word Embeddings\n",
    "\n",
    "**Word embeddings** are vector representations of words that capture their meanings and relationships in a mathematical space.\n",
    "\n",
    "### ğŸ”‘ Key Concepts\n",
    "**Word2Vec:**\n",
    "- Introduced by Google (2013).\n",
    "- Learns word vectors by predicting a word based on its context (CBOW) or predicting context based on a word (Skip-gram).\n",
    "- Example: `king - man + woman â‰ˆ queen`\n",
    "\n",
    "**GloVe (Global Vectors):**\n",
    "- Developed by Stanford.\n",
    "- Uses word co-occurrence statistics across a corpus to learn embeddings.\n",
    "- Captures both global and local statistics.\n",
    "\n",
    "### ğŸ§Š Static Embeddings\n",
    "- Each word has **one fixed vector**, regardless of the context it's used in.\n",
    "- Good at capturing general meaning and relationships (e.g., synonyms).\n",
    "- Limitation: Can't distinguish between different meanings of the same word (e.g., *apple* the fruit vs *Apple* the company).\n",
    "\n",
    "In the next section, weâ€™ll demonstrate this limitation through an interactive discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c13ee6",
   "metadata": {},
   "source": [
    "## ğŸ’¬ Interactive Discussion: Static vs Contextual Embeddings\n",
    "\n",
    "**Scenario:**\n",
    "Consider the word **\"apple\"** in the following sentences:\n",
    "\n",
    "1. \"She ate a juicy apple for lunch.\"\n",
    "2. \"Apple released a new iPhone this year.\"\n",
    "\n",
    "### â“ Discussion Questions:\n",
    "- Do the two sentences use the word *apple* in the same way?\n",
    "- What clues from the context help you understand the meaning?\n",
    "- Would a computer be able to tell the difference if it saw just the word *apple*?\n",
    "\n",
    "### ğŸ” Key Point:\n",
    "**Static embeddings** (like Word2Vec, GloVe) would treat *apple* the same in both sentences â€” they assign **one vector per word**.\n",
    "\n",
    "**Contextual embeddings** (like BERT) generate a **different vector for each usage**, based on surrounding words.\n",
    "\n",
    "This ability to capture **word meaning in context** is a major advancement in modern NLP, which weâ€™ll explore in detail in Session 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dea3c4",
   "metadata": {},
   "source": [
    "## ğŸ—£ï¸ Interactive Discussion: Static vs Contextual Embeddings\n",
    "\n",
    "Letâ€™s consider the word **\"apple\"** in two different sentences:\n",
    "\n",
    "1. *\"She ate a juicy apple after lunch.\"* ğŸ\n",
    "2. *\"Apple released a new iPhone this year.\"* ğŸ’»\n",
    "\n",
    "**Discussion Prompt:**\n",
    "- Do these two sentences use the word *apple* in the same way?\n",
    "- If you were a computer, and *apple* always had one vector, would that cause confusion?\n",
    "\n",
    "**ğŸ§Š Static Embeddings:**\n",
    "- Assign the **same vector** to *\"apple\"* in both sentences.\n",
    "- Cannot distinguish *fruit* from *company*.\n",
    "\n",
    "**ğŸ”¥ Contextual Embeddings (e.g., BERT):**\n",
    "- Generate a **different vector** for *\"apple\"* depending on the context.\n",
    "- In Sentence 1, \"apple\" would be closer to *banana*, *fruit*, *snack*.\n",
    "- In Sentence 2, \"apple\" would be closer to *iPhone*, *technology*, *company*.\n",
    "\n",
    "ğŸ‘‰ This limitation of static embeddings is a key motivation for the rise of **contextual word embeddings**, which weâ€™ll dive into in the next session."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
